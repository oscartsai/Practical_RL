{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:07:41,568] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# <your code here! Create an array to store action probabilities>\n",
    "policy = np.ones((n_states, n_actions))/n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # <pick action from policy (at random with probabilities)>\n",
    "        a = np.random.choice(n_actions, 1, p=policy[s])[0]\n",
    "\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        # <record prev state, action and add up reward to states,actions and total_reward accordingly>\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if r == 20:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 80 ms, total: 38.5 s\n",
      "Wall time: 38.5 s\n",
      "mean reward = -9501.28000\tthreshold = 125.0\n",
      "CPU times: user 11.4 s, sys: 0 ns, total: 11.4 s\n",
      "Wall time: 11.4 s\n",
      "mean reward = -3082.11600\tthreshold = 125.0\n",
      "CPU times: user 4.01 s, sys: 10 ms, total: 4.02 s\n",
      "Wall time: 4.03 s\n",
      "mean reward = -1160.59200\tthreshold = 125.0\n",
      "CPU times: user 1.95 s, sys: 0 ns, total: 1.95 s\n",
      "Wall time: 1.95 s\n",
      "mean reward = -539.65600\tthreshold = 125.0\n",
      "CPU times: user 1.08 s, sys: 0 ns, total: 1.08 s\n",
      "Wall time: 1.08 s\n",
      "mean reward = -277.80400\tthreshold = 125.0\n",
      "CPU times: user 790 ms, sys: 0 ns, total: 790 ms\n",
      "Wall time: 784 ms\n",
      "mean reward = -175.54400\tthreshold = 125.0\n",
      "CPU times: user 640 ms, sys: 0 ns, total: 640 ms\n",
      "Wall time: 639 ms\n",
      "mean reward = -119.09200\tthreshold = 125.0\n",
      "CPU times: user 500 ms, sys: 0 ns, total: 500 ms\n",
      "Wall time: 505 ms\n",
      "mean reward = -83.53200\tthreshold = 125.0\n",
      "CPU times: user 450 ms, sys: 0 ns, total: 450 ms\n",
      "Wall time: 454 ms\n",
      "mean reward = -65.66400\tthreshold = 125.0\n",
      "CPU times: user 430 ms, sys: 0 ns, total: 430 ms\n",
      "Wall time: 426 ms\n",
      "mean reward = -56.58400\tthreshold = 125.0\n",
      "CPU times: user 560 ms, sys: 0 ns, total: 560 ms\n",
      "Wall time: 558 ms\n",
      "mean reward = -104.94000\tthreshold = 125.0\n",
      "CPU times: user 1.71 s, sys: 0 ns, total: 1.71 s\n",
      "Wall time: 1.71 s\n",
      "mean reward = -443.84000\tthreshold = 125.0\n",
      "CPU times: user 1.04 s, sys: 0 ns, total: 1.04 s\n",
      "Wall time: 1.04 s\n",
      "mean reward = -239.86800\tthreshold = 125.0\n",
      "CPU times: user 1.12 s, sys: 0 ns, total: 1.12 s\n",
      "Wall time: 1.12 s\n",
      "mean reward = -264.24000\tthreshold = 125.0\n",
      "CPU times: user 980 ms, sys: 0 ns, total: 980 ms\n",
      "Wall time: 987 ms\n",
      "mean reward = -229.23600\tthreshold = 125.0\n",
      "CPU times: user 890 ms, sys: 0 ns, total: 890 ms\n",
      "Wall time: 888 ms\n",
      "mean reward = -208.73200\tthreshold = 125.0\n",
      "CPU times: user 1.45 s, sys: 0 ns, total: 1.45 s\n",
      "Wall time: 1.45 s\n",
      "mean reward = -372.54400\tthreshold = 125.0\n",
      "CPU times: user 620 ms, sys: 0 ns, total: 620 ms\n",
      "Wall time: 630 ms\n",
      "mean reward = -117.27200\tthreshold = 125.0\n",
      "CPU times: user 690 ms, sys: 0 ns, total: 690 ms\n",
      "Wall time: 689 ms\n",
      "mean reward = -138.51200\tthreshold = 125.0\n",
      "CPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\n",
      "Wall time: 1.06 s\n",
      "mean reward = -248.59200\tthreshold = 125.0\n",
      "CPU times: user 1.14 s, sys: 0 ns, total: 1.14 s\n",
      "Wall time: 1.14 s\n",
      "mean reward = -276.23600\tthreshold = 125.0\n",
      "CPU times: user 1.08 s, sys: 0 ns, total: 1.08 s\n",
      "Wall time: 1.08 s\n",
      "mean reward = -251.79200\tthreshold = 125.0\n",
      "CPU times: user 1.73 s, sys: 0 ns, total: 1.73 s\n",
      "Wall time: 1.73 s\n",
      "mean reward = -444.20000\tthreshold = 125.0\n",
      "CPU times: user 1.42 s, sys: 0 ns, total: 1.42 s\n",
      "Wall time: 1.42 s\n",
      "mean reward = -357.68800\tthreshold = 125.0\n",
      "CPU times: user 1.35 s, sys: 0 ns, total: 1.35 s\n",
      "Wall time: 1.35 s\n",
      "mean reward = -351.22800\tthreshold = 125.0\n",
      "CPU times: user 1.47 s, sys: 0 ns, total: 1.47 s\n",
      "Wall time: 1.48 s\n",
      "mean reward = -375.78000\tthreshold = 125.0\n",
      "CPU times: user 1.39 s, sys: 0 ns, total: 1.39 s\n",
      "Wall time: 1.39 s\n",
      "mean reward = -357.69600\tthreshold = 125.0\n",
      "CPU times: user 1.59 s, sys: 0 ns, total: 1.59 s\n",
      "Wall time: 1.59 s\n",
      "mean reward = -424.25600\tthreshold = 125.0\n",
      "CPU times: user 1.2 s, sys: 0 ns, total: 1.2 s\n",
      "Wall time: 1.19 s\n",
      "mean reward = -304.53600\tthreshold = 125.0\n",
      "CPU times: user 1.44 s, sys: 0 ns, total: 1.44 s\n",
      "Wall time: 1.43 s\n",
      "mean reward = -367.02000\tthreshold = 125.0\n",
      "CPU times: user 1.75 s, sys: 0 ns, total: 1.75 s\n",
      "Wall time: 1.75 s\n",
      "mean reward = -461.00000\tthreshold = 125.0\n",
      "CPU times: user 670 ms, sys: 0 ns, total: 670 ms\n",
      "Wall time: 675 ms\n",
      "mean reward = -139.34400\tthreshold = 125.0\n",
      "CPU times: user 630 ms, sys: 0 ns, total: 630 ms\n",
      "Wall time: 631 ms\n",
      "mean reward = -129.68000\tthreshold = 125.0\n",
      "CPU times: user 850 ms, sys: 0 ns, total: 850 ms\n",
      "Wall time: 846 ms\n",
      "mean reward = -191.66000\tthreshold = 125.0\n",
      "CPU times: user 990 ms, sys: 0 ns, total: 990 ms\n",
      "Wall time: 984 ms\n",
      "mean reward = -227.98400\tthreshold = 125.0\n",
      "CPU times: user 1.68 s, sys: 0 ns, total: 1.68 s\n",
      "Wall time: 1.68 s\n",
      "mean reward = -432.56000\tthreshold = 125.0\n",
      "CPU times: user 860 ms, sys: 0 ns, total: 860 ms\n",
      "Wall time: 868 ms\n",
      "mean reward = -198.43600\tthreshold = 125.0\n",
      "CPU times: user 760 ms, sys: 0 ns, total: 760 ms\n",
      "Wall time: 764 ms\n",
      "mean reward = -167.58400\tthreshold = 125.0\n",
      "CPU times: user 1.32 s, sys: 0 ns, total: 1.32 s\n",
      "Wall time: 1.32 s\n",
      "mean reward = -326.92000\tthreshold = 125.0\n",
      "CPU times: user 990 ms, sys: 0 ns, total: 990 ms\n",
      "Wall time: 991 ms\n",
      "mean reward = -242.24000\tthreshold = 125.0\n",
      "CPU times: user 1.89 s, sys: 0 ns, total: 1.89 s\n",
      "Wall time: 1.89 s\n",
      "mean reward = -513.98400\tthreshold = 125.0\n",
      "CPU times: user 860 ms, sys: 0 ns, total: 860 ms\n",
      "Wall time: 862 ms\n",
      "mean reward = -201.91600\tthreshold = 125.0\n",
      "CPU times: user 1.1 s, sys: 0 ns, total: 1.1 s\n",
      "Wall time: 1.1 s\n",
      "mean reward = -276.70800\tthreshold = 125.0\n",
      "CPU times: user 910 ms, sys: 0 ns, total: 910 ms\n",
      "Wall time: 918 ms\n",
      "mean reward = -215.14400\tthreshold = 125.0\n",
      "CPU times: user 760 ms, sys: 0 ns, total: 760 ms\n",
      "Wall time: 762 ms\n",
      "mean reward = -178.08400\tthreshold = 125.0\n",
      "CPU times: user 2.08 s, sys: 0 ns, total: 2.08 s\n",
      "Wall time: 2.09 s\n",
      "mean reward = -571.01600\tthreshold = 125.0\n",
      "CPU times: user 1.14 s, sys: 0 ns, total: 1.14 s\n",
      "Wall time: 1.14 s\n",
      "mean reward = -284.88400\tthreshold = 125.0\n",
      "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
      "Wall time: 2 s\n",
      "mean reward = -535.51600\tthreshold = 125.0\n",
      "CPU times: user 2.11 s, sys: 0 ns, total: 2.11 s\n",
      "Wall time: 2.11 s\n",
      "mean reward = -578.36800\tthreshold = 125.0\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "mean reward = -434.98400\tthreshold = 125.0\n",
      "CPU times: user 890 ms, sys: 0 ns, total: 890 ms\n",
      "Wall time: 887 ms\n",
      "mean reward = -209.91600\tthreshold = 125.0\n",
      "CPU times: user 1.41 s, sys: 0 ns, total: 1.41 s\n",
      "Wall time: 1.41 s\n",
      "mean reward = -355.37600\tthreshold = 125.0\n",
      "CPU times: user 620 ms, sys: 0 ns, total: 620 ms\n",
      "Wall time: 621 ms\n",
      "mean reward = -127.72000\tthreshold = 125.0\n",
      "CPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\n",
      "Wall time: 1.06 s\n",
      "mean reward = -254.39600\tthreshold = 125.0\n",
      "CPU times: user 1.02 s, sys: 0 ns, total: 1.02 s\n",
      "Wall time: 1.02 s\n",
      "mean reward = -239.66800\tthreshold = 125.0\n",
      "CPU times: user 1.99 s, sys: 0 ns, total: 1.99 s\n",
      "Wall time: 1.99 s\n",
      "mean reward = -523.70800\tthreshold = 125.0\n",
      "CPU times: user 1.05 s, sys: 0 ns, total: 1.05 s\n",
      "Wall time: 1.05 s\n",
      "mean reward = -251.42400\tthreshold = 125.0\n",
      "CPU times: user 1.68 s, sys: 0 ns, total: 1.68 s\n",
      "Wall time: 1.68 s\n",
      "mean reward = -447.77600\tthreshold = 125.0\n",
      "CPU times: user 1.07 s, sys: 10 ms, total: 1.08 s\n",
      "Wall time: 1.08 s\n",
      "mean reward = -272.95600\tthreshold = 125.0\n",
      "CPU times: user 1.36 s, sys: 0 ns, total: 1.36 s\n",
      "Wall time: 1.36 s\n",
      "mean reward = -350.62800\tthreshold = 125.0\n",
      "CPU times: user 970 ms, sys: 0 ns, total: 970 ms\n",
      "Wall time: 964 ms\n",
      "mean reward = -227.80400\tthreshold = 125.0\n",
      "CPU times: user 1.14 s, sys: 0 ns, total: 1.14 s\n",
      "Wall time: 1.13 s\n",
      "mean reward = -282.47200\tthreshold = 125.0\n",
      "CPU times: user 1.37 s, sys: 0 ns, total: 1.37 s\n",
      "Wall time: 1.36 s\n",
      "mean reward = -359.66000\tthreshold = 125.0\n",
      "CPU times: user 1.48 s, sys: 0 ns, total: 1.48 s\n",
      "Wall time: 1.48 s\n",
      "mean reward = -389.04400\tthreshold = 125.0\n",
      "CPU times: user 1.15 s, sys: 20 ms, total: 1.17 s\n",
      "Wall time: 1.17 s\n",
      "mean reward = -285.32800\tthreshold = 125.0\n",
      "CPU times: user 910 ms, sys: 0 ns, total: 910 ms\n",
      "Wall time: 911 ms\n",
      "mean reward = -210.44800\tthreshold = 125.0\n",
      "CPU times: user 1.19 s, sys: 10 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n",
      "mean reward = -286.24000\tthreshold = 125.0\n",
      "CPU times: user 1.23 s, sys: 90 ms, total: 1.32 s\n",
      "Wall time: 1.31 s\n",
      "mean reward = -308.61200\tthreshold = 125.0\n",
      "CPU times: user 1.08 s, sys: 40 ms, total: 1.12 s\n",
      "Wall time: 1.11 s\n",
      "mean reward = -260.54000\tthreshold = 125.0\n",
      "CPU times: user 1.54 s, sys: 10 ms, total: 1.55 s\n",
      "Wall time: 1.55 s\n",
      "mean reward = -397.93600\tthreshold = 125.0\n",
      "CPU times: user 1.29 s, sys: 30 ms, total: 1.32 s\n",
      "Wall time: 1.32 s\n",
      "mean reward = -317.50000\tthreshold = 125.0\n",
      "CPU times: user 900 ms, sys: 50 ms, total: 950 ms\n",
      "Wall time: 947 ms\n",
      "mean reward = -215.20800\tthreshold = 125.0\n",
      "CPU times: user 1.25 s, sys: 40 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n",
      "mean reward = -322.32800\tthreshold = 125.0\n",
      "CPU times: user 980 ms, sys: 20 ms, total: 1e+03 ms\n",
      "Wall time: 996 ms\n",
      "mean reward = -228.73600\tthreshold = 125.0\n",
      "CPU times: user 1.26 s, sys: 20 ms, total: 1.28 s\n",
      "Wall time: 1.29 s\n",
      "mean reward = -287.05200\tthreshold = 125.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 20 ms, total: 1.34 s\n",
      "Wall time: 1.34 s\n",
      "mean reward = -259.60400\tthreshold = 125.0\n",
      "CPU times: user 1.92 s, sys: 10 ms, total: 1.93 s\n",
      "Wall time: 1.93 s\n",
      "mean reward = -392.24800\tthreshold = 125.0\n",
      "CPU times: user 2.04 s, sys: 10 ms, total: 2.05 s\n",
      "Wall time: 2.05 s\n",
      "mean reward = -422.65600\tthreshold = 125.0\n",
      "CPU times: user 1.54 s, sys: 10 ms, total: 1.55 s\n",
      "Wall time: 1.55 s\n",
      "mean reward = -356.93200\tthreshold = 125.0\n",
      "CPU times: user 1.31 s, sys: 20 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n",
      "mean reward = -329.26800\tthreshold = 125.0\n",
      "CPU times: user 1.59 s, sys: 20 ms, total: 1.61 s\n",
      "Wall time: 1.61 s\n",
      "mean reward = -364.39200\tthreshold = 125.0\n",
      "CPU times: user 1.21 s, sys: 50 ms, total: 1.26 s\n",
      "Wall time: 1.26 s\n",
      "mean reward = -269.56000\tthreshold = 125.0\n",
      "CPU times: user 1.44 s, sys: 50 ms, total: 1.49 s\n",
      "Wall time: 1.5 s\n",
      "mean reward = -332.58000\tthreshold = 125.0\n",
      "CPU times: user 1.26 s, sys: 0 ns, total: 1.26 s\n",
      "Wall time: 1.26 s\n",
      "mean reward = -233.45200\tthreshold = 125.0\n",
      "CPU times: user 520 ms, sys: 0 ns, total: 520 ms\n",
      "Wall time: 523 ms\n",
      "mean reward = -83.18000\tthreshold = 125.0\n",
      "CPU times: user 2.18 s, sys: 100 ms, total: 2.28 s\n",
      "Wall time: 2.28 s\n",
      "mean reward = -442.02400\tthreshold = 125.0\n",
      "CPU times: user 1.62 s, sys: 10 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n",
      "mean reward = -356.08800\tthreshold = 125.0\n",
      "CPU times: user 1.85 s, sys: 0 ns, total: 1.85 s\n",
      "Wall time: 1.85 s\n",
      "mean reward = -469.65600\tthreshold = 125.0\n",
      "CPU times: user 1.89 s, sys: 0 ns, total: 1.89 s\n",
      "Wall time: 1.89 s\n",
      "mean reward = -487.54000\tthreshold = 125.0\n",
      "CPU times: user 900 ms, sys: 0 ns, total: 900 ms\n",
      "Wall time: 903 ms\n",
      "mean reward = -197.91600\tthreshold = 125.0\n",
      "CPU times: user 1.55 s, sys: 10 ms, total: 1.56 s\n",
      "Wall time: 1.56 s\n",
      "mean reward = -385.50000\tthreshold = 125.0\n",
      "CPU times: user 1.17 s, sys: 50 ms, total: 1.22 s\n",
      "Wall time: 1.22 s\n",
      "mean reward = -290.10000\tthreshold = 125.0\n",
      "CPU times: user 1.35 s, sys: 0 ns, total: 1.35 s\n",
      "Wall time: 1.34 s\n",
      "mean reward = -329.29600\tthreshold = 125.0\n",
      "CPU times: user 950 ms, sys: 20 ms, total: 970 ms\n",
      "Wall time: 968 ms\n",
      "mean reward = -217.94400\tthreshold = 125.0\n",
      "CPU times: user 760 ms, sys: 40 ms, total: 800 ms\n",
      "Wall time: 803 ms\n",
      "mean reward = -134.31200\tthreshold = 125.0\n",
      "CPU times: user 820 ms, sys: 0 ns, total: 820 ms\n",
      "Wall time: 825 ms\n",
      "mean reward = -169.84400\tthreshold = 125.0\n",
      "CPU times: user 670 ms, sys: 40 ms, total: 710 ms\n",
      "Wall time: 707 ms\n",
      "mean reward = -136.97200\tthreshold = 125.0\n",
      "CPU times: user 1.46 s, sys: 10 ms, total: 1.47 s\n",
      "Wall time: 1.47 s\n",
      "mean reward = -366.90800\tthreshold = 125.0\n",
      "CPU times: user 1.43 s, sys: 50 ms, total: 1.48 s\n",
      "Wall time: 1.48 s\n",
      "mean reward = -364.95200\tthreshold = 125.0\n",
      "CPU times: user 990 ms, sys: 20 ms, total: 1.01 s\n",
      "Wall time: 1.02 s\n",
      "mean reward = -236.92800\tthreshold = 125.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # <generate n_samples sessions>\n",
    "    %time sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    # <select percentile of your samples>\n",
    "    threshold = int(n_samples*percentile/100)\n",
    "    ids = np.argsort(batch_rewards)[-threshold:]\n",
    "    \n",
    "    # <select states from sessions where rewards are above threshold>\n",
    "    elite_states = batch_states[ids]\n",
    "    \n",
    "    # <select actions from sessions where rewards are above threshold>\n",
    "    elite_actions = batch_actions[ids]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    # <count all state-action occurences in elite_states and elite_actions>\n",
    "    for i in range(len(elite_states)):\n",
    "        elite_counts[elite_states[i], elite_actions[i]] += 1\n",
    "\n",
    "    policy = elite_counts/np.sum(elite_counts, axis=1, keepdims=True)\n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:54:13,942] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f298ee3c7b8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEkpJREFUeJzt3X+s39Vdx/HnS8pgbtPCuDa1Pyy66oLGFbwyyBaDkCmg\nsZjoAhpHFpKLCUu2uKigiW6JJJro0EUlVmHrzBxDtklD0Ikdidkfg7Vb17V0uLutpG0KLRuwzUW0\n7O0f95R9V257v/d+77e39+z5SD75fj7ncz6f7znwzet+7rnn9JuqQpLUn+9b6gZIksbDgJekThnw\nktQpA16SOmXAS1KnDHhJ6tTYAj7J1UkeTzKd5NZxvY8kaXYZxzz4JGcB/wW8CTgIfBq4oaoeW/Q3\nkyTNalxP8JcC01X15ar6X+AeYPOY3kuSNIsVY7rvGuDAwPFB4PUnq3zBBRfUhg0bxtQUSVp+9u/f\nz9NPP51R7jGugJ9TkilgCmD9+vXs2LFjqZoiSWecycnJke8xriGaQ8C6geO1rexFVbWlqiaranJi\nYmJMzZCk713jCvhPAxuTXJjkZcD1wLYxvZckaRZjGaKpqmNJ3gZ8HDgLuLuq9o7jvSRJsxvbGHxV\nPQg8OK77S5JOzZWsktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjpl\nwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6NdJX9iXZD3wDeAE4VlWTSc4HPgxs\nAPYDb66qZ0ZrpiRpvhbjCf7nq2pTVU2241uB7VW1EdjejiVJp9k4hmg2A1vb/lbgujG8hyRpDqMG\nfAH/nmRnkqlWtqqqDrf9J4FVI76HJGkBRhqDB95YVYeS/BDwUJIvDJ6sqkpSs13YfiBMAaxfv37E\nZkiSTjTSE3xVHWqvR4CPAZcCTyVZDdBej5zk2i1VNVlVkxMTE6M0Q5I0iwUHfJJXJHnV8X3gF4A9\nwDbgxlbtRuD+URspSZq/UYZoVgEfS3L8Pv9UVf+W5NPAvUluAp4A3jx6MyVJ87XggK+qLwOvm6X8\nq8BVozRKkjQ6V7JKUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQB\nL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnZoz4JPcneRIkj0DZecneSjJ\nF9vrea08Sd6bZDrJ7iSXjLPxkqSTG+YJ/v3A1SeU3Qpsr6qNwPZ2DHANsLFtU8Cdi9NMSdJ8zRnw\nVfWfwNdOKN4MbG37W4HrBso/UDM+BaxMsnqxGitJGt5Cx+BXVdXhtv8ksKrtrwEODNQ72MpeIslU\nkh1Jdhw9enSBzZAknczIf2StqgJqAddtqarJqpqcmJgYtRmSpBMsNOCfOj700l6PtPJDwLqBemtb\nmSTpNFtowG8Dbmz7NwL3D5S/pc2muQx4bmAoR5J0Gq2Yq0KSDwFXABckOQj8MfCnwL1JbgKeAN7c\nqj8IXAtMA98C3jqGNkuShjBnwFfVDSc5ddUsdQu4ZdRGSZJG50pWSeqUAS9JnTLgJalTBrwkdcqA\nl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ\n6pQBL0mdmjPgk9yd5EiSPQNl70pyKMmutl07cO62JNNJHk/yi+NquCTp1IZ5gn8/cPUs5XdU1aa2\nPQiQ5CLgeuAn2zV/m+SsxWqsJGl4cwZ8Vf0n8LUh77cZuKeqnq+qrwDTwKUjtE+StECjjMG/Lcnu\nNoRzXitbAxwYqHOwlb1EkqkkO5LsOHr06AjNkCTNZqEBfyfwY8Am4DDwF/O9QVVtqarJqpqcmJhY\nYDMkSSezoICvqqeq6oWq+jbw93xnGOYQsG6g6tpWJkk6zRYU8ElWDxz+KnB8hs024Pok5yS5ENgI\nPDpaEyVJC7FirgpJPgRcAVyQ5CDwx8AVSTYBBewHbgaoqr1J7gUeA44Bt1TVC+NpuiTpVOYM+Kq6\nYZbiu05R/3bg9lEaJUkanStZJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqfmnCYpfa/YueXml5T9\nzNTfLUFLpMXhE7wkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8xOyLnKTlzoCX\npE4Z8JLUqTkDPsm6JA8neSzJ3iRvb+XnJ3koyRfb63mtPEnem2Q6ye4kl4y7E5KklxrmCf4Y8M6q\nugi4DLglyUXArcD2qtoIbG/HANcAG9s2Bdy56K2WJM1pzoCvqsNV9Zm2/w1gH7AG2AxsbdW2Ate1\n/c3AB2rGp4CVSVYvesslSac0rzH4JBuAi4FHgFVVdbidehJY1fbXAAcGLjvYyk6811SSHUl2HD16\ndJ7NliTNZeiAT/JK4CPAO6rq64PnqqqAms8bV9WWqpqsqsmJiYn5XCpJGsJQAZ/kbGbC/YNV9dFW\n/NTxoZf2eqSVHwLWDVy+tpVJkk6jYWbRBLgL2FdV7xk4tQ24se3fCNw/UP6WNpvmMuC5gaEcSdJp\nMsxX9r0B+C3g80l2tbI/AP4UuDfJTcATwJvbuQeBa4Fp4FvAWxe1xZKkocwZ8FX1SSAnOX3VLPUL\nuGXEdkmSRuRKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAl\nqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnRrmS7fXJXk4yWNJ9iZ5eyt/V5JDSXa1\n7dqBa25LMp3k8SS/OM4OSJJmN8yXbh8D3llVn0nyKmBnkofauTuq6s8HKye5CLge+Engh4H/SPLj\nVfXCYjZcknRqcz7BV9XhqvpM2/8GsA9Yc4pLNgP3VNXzVfUVYBq4dDEaK0ka3rzG4JNsAC4GHmlF\nb0uyO8ndSc5rZWuAAwOXHeTUPxAkSWMwdMAneSXwEeAdVfV14E7gx4BNwGHgL+bzxkmmkuxIsuPo\n0aPzuVSSNIShAj7J2cyE+wer6qMAVfVUVb1QVd8G/p7vDMMcAtYNXL62lX2XqtpSVZNVNTkxMTFK\nHyRJsxhmFk2Au4B9VfWegfLVA9V+FdjT9rcB1yc5J8mFwEbg0cVrsiRpGMPMonkD8FvA55PsamV/\nANyQZBNQwH7gZoCq2pvkXuAxZmbg3OIMGkk6/eYM+Kr6JJBZTj14imtuB24foV2SpBG5klWSOmXA\nS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBry6\nlWTobTaTN28Z6XppqRnwktSpYb7wQ/qe8MDhqRf3f3n1liVsibQ4fIKX+O5wn+1YWo4MeEnq1DBf\nun1ukkeTfC7J3iTvbuUXJnkkyXSSDyd5WSs/px1Pt/MbxtsFSdJshnmCfx64sqpeB2wCrk5yGfBn\nwB1V9RrgGeCmVv8m4JlWfkerJ53RThxzdwxePRjmS7cL+GY7PLttBVwJ/EYr3wq8C7gT2Nz2Ae4D\n/jpJ2n2kM9LkzVuA74T6u5asJdLiGWoWTZKzgJ3Aa4C/Ab4EPFtVx1qVg8Catr8GOABQVceSPAe8\nGnj6ZPffuXOnc4m1rPn51ZloqICvqheATUlWAh8DXjvqGyeZAqYA1q9fzxNPPDHqLaXvcjpD119Q\ntdgmJydHvse8ZtFU1bPAw8DlwMokx39ArAUOtf1DwDqAdv4Hga/Ocq8tVTVZVZMTExMLbL4k6WSG\nmUUz0Z7cSfJy4E3APmaC/tdatRuB+9v+tnZMO/8Jx98l6fQbZohmNbC1jcN/H3BvVT2Q5DHgniR/\nAnwWuKvVvwv4xyTTwNeA68fQbknSHIaZRbMbuHiW8i8Dl85S/j/Ary9K6yRJC+ZKVknqlAEvSZ0y\n4CWpU/5zweqWk7f0vc4neEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQB\nL0mdMuAlqVMGvCR1yoCXpE4Z8JLUqWG+dPvcJI8m+VySvUne3crfn+QrSXa1bVMrT5L3JplOsjvJ\nJePuhCTppYb59+CfB66sqm8mORv4ZJJ/bed+t6ruO6H+NcDGtr0euLO9SpJOozmf4GvGN9vh2W07\n1TcpbAY+0K77FLAyyerRmypJmo+hxuCTnJVkF3AEeKiqHmmnbm/DMHckOaeVrQEODFx+sJVJkk6j\noQK+ql6oqk3AWuDSJD8F3Aa8FvhZ4Hzg9+fzxkmmkuxIsuPo0aPzbLYkaS7zmkVTVc8CDwNXV9Xh\nNgzzPPA+4NJW7RCwbuCyta3sxHttqarJqpqcmJhYWOslSSc1zCyaiSQr2/7LgTcBXzg+rp4kwHXA\nnnbJNuAtbTbNZcBzVXV4LK2XJJ3UMLNoVgNbk5zFzA+Ee6vqgSSfSDIBBNgF/Har/yBwLTANfAt4\n6+I3W5I0lzkDvqp2AxfPUn7lSeoXcMvoTZMkjcKVrJLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalT\nBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXA\nS1Knhg74JGcl+WySB9rxhUkeSTKd5MNJXtbKz2nH0+38hvE0XZJ0KvN5gn87sG/g+M+AO6rqNcAz\nwE2t/CbgmVZ+R6snSTrNhgr4JGuBXwL+oR0HuBK4r1XZClzX9je3Y9r5q1p9SdJptGLIen8J/B7w\nqnb8auDZqjrWjg8Ca9r+GuAAQFUdS/Jcq//04A2TTAFT7fD5JHsW1IMz3wWc0PdO9Nov6Ldv9mt5\n+ZEkU1W1ZaE3mDPgk/wycKSqdia5YqFvdKLW6C3tPXZU1eRi3ftM0mvfeu0X9Ns3+7X8JNlBy8mF\nGOYJ/g3AryS5FjgX+AHgr4CVSVa0p/i1wKFW/xCwDjiYZAXwg8BXF9pASdLCzDkGX1W3VdXaqtoA\nXA98oqp+E3gY+LVW7Ubg/ra/rR3Tzn+iqmpRWy1JmtMo8+B/H/idJNPMjLHf1crvAl7dyn8HuHWI\ney34V5BloNe+9dov6Ldv9mv5Galv8eFakvrkSlZJ6tSSB3ySq5M83la+DjOcc0ZJcneSI4PTPJOc\nn+ShJF9sr+e18iR5b+vr7iSXLF3LTy3JuiQPJ3ksyd4kb2/ly7pvSc5N8miSz7V+vbuVd7Eyu9cV\n50n2J/l8kl1tZsmy/ywCJFmZ5L4kX0iyL8nli9mvJQ34JGcBfwNcA1wE3JDkoqVs0wK8H7j6hLJb\nge1VtRHYznf+DnENsLFtU8Cdp6mNC3EMeGdVXQRcBtzS/t8s9749D1xZVa8DNgFXJ7mMflZm97zi\n/OeratPAlMjl/lmEmRmJ/1ZVrwVex8z/u8XrV1Ut2QZcDnx84Pg24LalbNMC+7EB2DNw/Diwuu2v\nBh5v+38H3DBbvTN9Y2aW1Jt66hvw/cBngNczs1BmRSt/8XMJfBy4vO2vaPWy1G0/SX/WtkC4EngA\nSA/9am3cD1xwQtmy/iwyM4X8Kyf+d1/Mfi31EM2Lq16bwRWxy9mqqjrc9p8EVrX9Zdnf9uv7xcAj\ndNC3NoyxCzgCPAR8iSFXZgPHV2afiY6vOP92Ox56xTlndr8ACvj3JDvbKnhY/p/FC4GjwPvasNo/\nJHkFi9ivpQ747tXMj9plO1UpySuBjwDvqKqvD55brn2rqheqahMzT7yXAq9d4iaNLAMrzpe6LWPy\nxqq6hJlhiluS/NzgyWX6WVwBXALcWVUXA//NCdPKR+3XUgf88VWvxw2uiF3OnkqyGqC9Hmnly6q/\nSc5mJtw/WFUfbcVd9A2gqp5lZsHe5bSV2e3UbCuzOcNXZh9fcb4fuIeZYZoXV5y3OsuxXwBU1aH2\negT4GDM/mJf7Z/EgcLCqHmnH9zET+IvWr6UO+E8DG9tf+l/GzErZbUvcpsUwuJr3xFW+b2l/Db8M\neG7gV7EzSpIws2htX1W9Z+DUsu5bkokkK9v+y5n5u8I+lvnK7Op4xXmSVyR51fF94BeAPSzzz2JV\nPQkcSPITregq4DEWs19nwB8argX+i5lx0D9c6vYsoP0fAg4D/8fMT+SbmBnL3A58EfgP4PxWN8zM\nGvoS8Hlgcqnbf4p+vZGZXw13A7vadu1y7xvw08BnW7/2AH/Uyn8UeBSYBv4ZOKeVn9uOp9v5H13q\nPgzRxyuAB3rpV+vD59q293hOLPfPYmvrJmBH+zz+C3DeYvbLlayS1KmlHqKRJI2JAS9JnTLgJalT\nBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqf+H1ijfMlWCpxLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f298ed26dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0]\n",
    "        \n",
    "        # <sample action with such probabilities>\n",
    "        a = np.random.choice(n_actions, 1, p=probs)[0]\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,) (70,)\n",
      "(5735, 4) (5735,)\n",
      "mean reward = 68.97000\tthreshold = 70.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    # <select percentile of your samples>\n",
    "    threshold = int(n_samples*percentile/100)\n",
    "    ids = np.argsort(batch_rewards)[-threshold:]\n",
    "    \n",
    "    # <select states from sessions where rewards are above threshold>\n",
    "    elite_states = batch_states[ids]\n",
    "    # <select actions from sessions where rewards are above threshold>\n",
    "    elite_actions = batch_actions[ids]\n",
    "    # print(elite_states.shape, elite_actions.shape)\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    # print(elite_states.shape, elite_actions.shape)\n",
    "    # <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.fit(elite_states, elite_actions);\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 14:48:38,619] Making new env: CartPole-v0\n",
      "[2017-07-24 14:48:38,624] Creating monitor directory videos\n",
      "[2017-07-24 14:48:38,649] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.40.video000000.mp4\n",
      "[2017-07-24 14:48:41,530] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.40.video000001.mp4\n",
      "[2017-07-24 14:48:44,026] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.40.video000008.mp4\n",
      "[2017-07-24 14:48:46,582] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.40.video000027.mp4\n",
      "[2017-07-24 14:48:49,908] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.40.video000064.mp4\n",
      "[2017-07-24 14:48:52,978] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/week1/videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-31 11:51:15,450] Making new env: BreakoutDeterministic-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4bb0da1d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nm5klwoFuZpaI/wENFojxpHLWRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4bfdbcfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4baffa198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAu6mVki/g+9SxOhER8x8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4bb083400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4bafddb70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4bb04b400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
