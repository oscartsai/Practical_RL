{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to this course and want more instructions on how to set up environement and all the libs (docker / windows / gpu / blas / etc.), you could read [vital instructions here](https://github.com/yandexdataschool/Practical_RL/issues/1#issue-202648393). \n",
    "\n",
    "Please make sure that your have bleeding edge versions of Theano, Lasagne and Agentnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General purpose libs import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from IPython.core import  display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you have  GPU uncomment the line below \n",
    "# %env THEANO_FLAGS=device=gpu0, floatX=float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal collection of a gentleman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from agentnet.agent import Agent\n",
    "from agentnet.experiments.openai_gym.wrappers import PreprocessImage\n",
    "from agentnet.memory import WindowAugmentation, LSTMCell, GRUCell\n",
    "from agentnet.target_network import TargetNetwork\n",
    "from agentnet.resolver import EpsilonGreedyResolver, ProbabilisticResolver\n",
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T \n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import DenseLayer, Conv2DLayer, InputLayer, NonlinearityLayer\n",
    "from lasagne.layers import batch_norm, get_all_params, get_output, reshape, concat, dropout\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, elu, tanh, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample image, and crop it, showing only the most useful part of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(\"KungFuMaster-v0\")\n",
    "    env = PreprocessImage(env, height=64, width=64, grayscale=True, crop=lambda img: img[60:-30, 7:] )\n",
    "    return env "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for tracking performance while training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_and_plot(rewards, epoch_counter, pool, target_score, th_times, loop_times):\n",
    "    rewards[epoch_counter] = np.mean(pool.evaluate(n_games=N_EVAL_GAMES,record_video=False, verbose=False))\n",
    "    info_string = \"Time (DL/All) {:.1f}/{:.1f}  epoch={}, mean_score={:.2f}\"\n",
    "    info_string = info_string.format(np.mean(th_times), np.mean(loop_times), \n",
    "                                     epoch_counter, np.mean(rewards[epoch_counter]))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot([rewards[i] for i in sorted(rewards.keys())])\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Mean reward over evaluation games\")\n",
    "    plt.title(info_string)\n",
    "    plt.show()\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 14:31:04,340] Making new env: KungFuMaster-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('KungFuMaster-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9dfbcd080>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS+QkMkv0/xbQ0z0/AyvAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9e1dfd198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 14:31:04,862] Making new env: KungFuMaster-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd9dfb332b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9dfc18f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_env()\n",
    "plt.imshow(np.squeeze(env.reset()), interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global constants definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyperparameters (except number of layers and neurons) are declared here as upper case letters along with global varaibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_ACTIONS = env.action_space.n\n",
    "OBS_SHAPE = env.observation_space.shape \n",
    "OBS_CHANNELS, OBS_HEIGHT, OBS_WIDTH = OBS_SHAPE\n",
    "\n",
    "# These 4 constanst were shown to lead to nearly state of the art on kung-fu master game\n",
    "N_SIMULTANEOUS_GAMES = 10  # this is also known as number of agents in exp_replay_pool\n",
    "SEQ_LENGTH = 25\n",
    "\n",
    "EVAL_EVERY_N_ITER = 100\n",
    "N_EVAL_GAMES = 2\n",
    "\n",
    "N_FRAMES_IN_BUFFER = 4 # number of consequent frames to feed in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation_layer = InputLayer((None,) + OBS_SHAPE)\n",
    "prev_wnd = InputLayer([None, N_FRAMES_IN_BUFFER, OBS_CHANNELS, OBS_HEIGHT, OBS_WIDTH])\n",
    "new_wnd = WindowAugmentation(observation_layer, prev_wnd)\n",
    "wnd_reshape = reshape(new_wnd, [-1,  N_FRAMES_IN_BUFFER * OBS_CHANNELS, OBS_HEIGHT, OBS_WIDTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "# provide the main body of the network : first three convolutional layers and dense one on top \n",
    "# you may want to change nonlinearity - feel free to do this \n",
    "# note that we have changed filter size here because of reduced image width and height compared to those in papers\n",
    "conv1 = Conv2DLayer(wnd_reshape,\n",
    "                    num_filters=32,\n",
    "                    filter_size=(3, 3),\n",
    "                    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                    W=lasagne.init.GlorotUniform())\n",
    "\n",
    "conv2 = Conv2DLayer(conv1,\n",
    "                    num_filters=32,\n",
    "                    filter_size=(3, 3),\n",
    "                    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                    W=lasagne.init.GlorotUniform())\n",
    "\n",
    "conv3 = Conv2DLayer(conv2,\n",
    "                    num_filters=32,\n",
    "                    filter_size=(3, 3),\n",
    "                    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                    W=lasagne.init.GlorotUniform())\n",
    "\n",
    "dense = DenseLayer(conv3,\n",
    "                   num_units=64,\n",
    "                   nonlinearity=lasagne.nonlinearities.rectify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "# YOUR CODE HERE\n",
    "# define 256 neuron LSTM cell:\n",
    "# - define two input layers each of n_lstm_cells (maybe 256 is a good baseline) neurons \n",
    "# - feed into `LSTMcell` this two layers and \n",
    "#   input layer (last `Dense` in case of A2C+LSTM) as additional third parameter\n",
    "\n",
    "prev_cell = InputLayer((None, HIDDEN_SIZE))\n",
    "prev_out = InputLayer((None, HIDDEN_SIZE))\n",
    "\n",
    "new_cell,new_out = LSTMCell(prev_cell,\n",
    "                            prev_out,\n",
    "                            dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neck_layer = concat([dense, new_out]) # network neck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \n",
    "# define actors head as \n",
    "# - logits_layer – dense(neck) with nonlinearity=None \n",
    "# - policy layer – softmax over logits_layer\n",
    "logits_layer = DenseLayer(neck_layer,\n",
    "                          num_units=N_ACTIONS,\n",
    "                          nonlinearity=None)\n",
    "\n",
    "policy_layer = NonlinearityLayer(logits_layer, softmax)\n",
    "\n",
    "action_layer = ProbabilisticResolver(policy_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# critic head\n",
    "V_layer = DenseLayer(neck_layer, 1, nonlinearity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# `observation_layers` is input layer to NN, as usual\n",
    "# `policy_estimators` should include 1) logits_layer and 2) V_layer \n",
    "# `agent_states` is a dictionary of {new_value: old_value}. You should bother to update \n",
    "#    a) prev window (input buffer, prev_wnd)  b) previous LSTM cell state  c) output of LSTM cell \n",
    "# `action_layers` is action_layer, as usual : ) \n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(logits_layer, V_layer),\n",
    "              agent_states={new_wnd:prev_wnd, new_cell:prev_cell, new_out:prev_out},\n",
    "              action_layers=action_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 14:31:05,712] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:05,944] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:06,150] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:06,351] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:06,555] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:06,763] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:06,981] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:07,182] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:07,388] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:07,593] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:31:14,292] We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "# may need to adjust (increasing N_SIMULTANEOUS_GAMES is usually a good idea)\n",
    "pool = EnvPool(agent, make_env, n_games=N_SIMULTANEOUS_GAMES) \n",
    "replay = pool.experience_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, action_seq, (logits_seq, V_seq) = agent.get_sessions(\n",
    "    replay, \n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute pi(a|s) and log(pi(a|s)) manually [use logsoftmax]\n",
    "# we can't guarantee that theano optimizes logsoftmax automatically since it's still in dev \n",
    "# for more info see (https://github.com/Theano/Theano/issues/2944 of 2015 year)\n",
    "\n",
    "# logits_seq.shape is (batch_size, SEQ_LENGTH, N_ACTIONS)\n",
    "logits_flat = logits_seq.reshape([-1, N_ACTIONS])\n",
    "policy_seq = T.nnet.softmax(logits_flat).reshape(logits_seq.shape)\n",
    "logpolicy_seq = T.nnet.logsoftmax(logits_flat).reshape(logits_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get policy gradient\n",
    "from agentnet.learning import a2c\n",
    "elwise_actor_loss, elwise_critic_loss = a2c.get_elementwise_objective(\n",
    "    policy=logpolicy_seq,\n",
    "    treat_policy_as_logpolicy=True,\n",
    "    state_values=V_seq[:, :, 0],\n",
    "    actions=replay.actions[0],\n",
    "    rewards=replay.rewards/10, \n",
    "    is_alive=replay.is_alive,\n",
    "    gamma_or_gammas=0.99,\n",
    "    n_steps=None,\n",
    "    return_separate=True\n",
    ")\n",
    "        \n",
    "# add losses with magic numbers \n",
    "# (you can change them more or less harmlessly, this usually just makes learning faster/slower)\n",
    "# actor and critic multipliers were selected guided by prior knowledge  \n",
    "# entropy / regularization multipliers were tuned with logscale gridsearch\n",
    "# NB: regularization affects exploration\n",
    "reg_logits = T.mean(logits_seq ** 2)\n",
    "reg_entropy = T.mean(T.sum(policy_seq * logpolicy_seq, axis=-1))\n",
    "loss = 0.1 * elwise_actor_loss.mean() + 0.25 * elwise_critic_loss.mean() + 1e-3 * reg_entropy + 1e-3 * reg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 14:32:41,446] We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "# Compute weight updates, clip by norm for stability \n",
    "weights = lasagne.layers.get_all_params([V_layer, policy_layer], trainable=True)\n",
    "grads = T.grad(loss, weights)\n",
    "grads = lasagne.updates.total_norm_constraint(grads, 10)\n",
    "updates = lasagne.updates.adam(grads, weights)\n",
    "train_step = theano.function([], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 # starting epoch\n",
    "rewards = {} # full game rewards\n",
    "target_score = 10000\n",
    "loss, eval_rewards = 0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-09 14:33:42,886] Making new env: KungFuMaster-v0\n",
      "[2017-11-09 14:33:43,122] Creating monitor directory ./records\n",
      "[2017-11-09 14:36:15,438] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/week7/records')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_reward = np.mean(pool.evaluate(n_games=5, record_video=False, verbose=False))\n",
    "untrained_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF you feel disgust about stderr messages due to pool.evaluate() execution \n",
    "# which pollutes output of jupyter cell, you could do one of the following:\n",
    "# 1. use warnings.filterwarnings(\"ignore\")\n",
    "# 2. use cell magic %%capture\n",
    "# 3. simply redirect stderr to /dev/null with command\n",
    "#    import os, sys\n",
    "#    stder_old = sys.stderr\n",
    "#    sys.stderr = open(os.devnull, 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6903a142a68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloop_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/agentnet/experiments/openai_gym/pool.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n_steps, append, max_size, add_last_observation, preprocess)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Get interaction sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         observation_tensor, action_tensor, reward_tensor, _, is_alive_tensor, _ = self.interact(n_steps=n_steps,\n\u001b[0;32m--> 194\u001b[0;31m                                                                                                 add_last_observation=add_last_observation)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mobservation_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_alive_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreceding_memory_states\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/agentnet/experiments/openai_gym/pool.py\u001b[0m in \u001b[0;36minteract\u001b[0;34m(self, n_steps, verbose, add_last_observation)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_last_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_memory_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_memory_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "th_times, loop_times = [], []\n",
    "for i in range(2000):  \n",
    "    loop_starts = timer()\n",
    "    pool.update(SEQ_LENGTH) \n",
    "    \n",
    "    train_starts = timer()\n",
    "    \n",
    "    # YOUR CODE HERE : train network (actor and critic)\n",
    "    train_step()\n",
    "    \n",
    "    th_times.append(timer() - train_starts)\n",
    "    epoch_counter  +=1\n",
    "    loop_times.append(timer() - loop_starts)\n",
    "    \n",
    "    #You may want to set EVAL_EVERY_N_ITER=1 for the time being\n",
    "    if epoch_counter % EVAL_EVERY_N_ITER==0: \n",
    "        eval_and_plot(rewards, epoch_counter, pool, target_score, th_times, loop_times)\n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"VICTORY!\")\n",
    "            break\n",
    "        th_times, loop_times = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_and_plot(rewards, epoch_counter, pool, target_score, th_times, loop_times)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
