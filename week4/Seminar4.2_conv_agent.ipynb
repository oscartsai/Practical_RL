{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[this demo requires doom installed either from gym-pool or from [ppaquette's repo](https://github.com/ppaquette/gym-doom)]\n",
    "\n",
    "## Basic Doom demo\n",
    "\n",
    "This demo solves DoomBasic env with a simple q-learning with experience replay.\n",
    "\n",
    "Video observation forces you to use ```CNN```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu, floatX=float32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=device=cpu, floatX=float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 07:31:01,894] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from gym.wrappers import SkipWrapper\n",
    "from ppaquette_gym_doom.wrappers.action_space import ToDiscrete\n",
    "from agentnet.experiments.openai_gym.wrappers import PreprocessImage\n",
    "GAME_NAME = 'ppaquette/DoomBasic-v0'\n",
    "\n",
    "make_env = lambda: PreprocessImage(SkipWrapper(4)(ToDiscrete(\"minimal\")(gym.make(GAME_NAME))),\n",
    "                                   width=80, height=80, grayscale=True)\n",
    "\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape: (1, 80, 80)\n",
      "n_action: 4\n"
     ]
    }
   ],
   "source": [
    "#global params.\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "SEQ_LENGTH = 10\n",
    "FRAME_NUMBER = 4\n",
    "print(\"observation shape:\", observation_shape)\n",
    "print(\"n_action:\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0 False\n",
      "(1, 80, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f50b5936e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f50b68b8390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, r, done, _ = env.step(1)\n",
    "print(r, done)\n",
    "print(np.shape(obs))\n",
    "plt.imshow(obs[0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu, floatX=float32\n"
     ]
    }
   ],
   "source": [
    "#setup and import theano/lasagne. Prefer GPU\n",
    "%env THEANO_FLAGS=device=cpu, floatX=float32\n",
    "\n",
    "import theano, lasagne\n",
    "import theano.tensor as T\n",
    "from lasagne.layers import *\n",
    "\n",
    "from agentnet.memory import WindowAugmentation, LSTMCell\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.target_network import TargetNetwork\n",
    "from agentnet.experiments.openai_gym.pool import EnvPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of observation layer (None, 1, 80, 80)\n",
      "shape of prev wnd (None, 4, 1, 80, 80)\n",
      "reshape to (-1, 4, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "#observation\n",
    "observation_layer = InputLayer((None,)+observation_shape,)\n",
    "print(\"shape of observation layer\", (None,)+observation_shape)\n",
    "\n",
    "#4-tick window over images\n",
    "prev_wnd = InputLayer((None, FRAME_NUMBER)+observation_shape)\n",
    "print(\"shape of prev wnd\", (None, FRAME_NUMBER)+observation_shape)\n",
    "new_wnd = WindowAugmentation(observation_layer, prev_wnd)\n",
    "        \n",
    "#reshape to (frame, h,w). If you don't use grayscale, 4 should become 12.\n",
    "wnd_reshape = reshape(new_wnd, (-1, FRAME_NUMBER*observation_shape[0])+observation_shape[1:])\n",
    "print(\"reshape to\", (-1, FRAME_NUMBER*observation_shape[0])+observation_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define your NN which probably solve the environment. \n",
    "\n",
    "#### Tips:\n",
    "1. Main component are likely to be ```Conv2D``` and ```Pool2DLayer```\n",
    "2. Batch normalization here might speeds up training but may get unstable if you use small experience replay buffer\n",
    "3. Last layers should be Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.nonlinearities import elu, tanh, softmax\n",
    "net = {}\n",
    "net['conv1_1'] = Conv2DLayer(wnd_reshape, 32, 3, pad=1)\n",
    "net['conv1_2'] = Conv2DLayer(net['conv1_1'], 32, 3, pad=1)\n",
    "net['pool1'] = Pool2DLayer(net['conv1_2'], 2)\n",
    "net['conv2_1'] = Conv2DLayer(net['pool1'], 32, 3, pad=1)\n",
    "net['conv2_2'] = Conv2DLayer(net['conv2_1'], 32, 3, pad=1)\n",
    "net['pool2'] = Pool2DLayer(net['conv2_2'], 2)\n",
    "dense = DenseLayer(net['pool2'], num_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#baseline for all qvalues\n",
    "qvalues_layer = DenseLayer(dense, num_units=n_actions, nonlinearity=None, name='qval')\n",
    "        \n",
    "#sample actions proportionally to policy_layer\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetnet = TargetNetwork(qvalues_layer)\n",
    "qvalues_old = targetnet.output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(qvalues_layer, qvalues_old),\n",
    "              agent_states={new_wnd: prev_wnd},\n",
    "              action_layers=action_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b, W, b, qval.W, qval.b]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer, trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 07:31:03,235] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 07:31:03,255] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 07:31:03,280] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 07:31:03,298] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "pool = EnvPool(agent,\n",
    "               make_env, \n",
    "               n_games=4, #parallel games (only 1 so far)\n",
    "               max_size=1000) #experience replay pool holding last 1k sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 3 3 3 3 0]\n",
      " [3 1 3 3 1 3 3]\n",
      " [3 1 3 3 3 3 1]\n",
      " [3 1 3 3 3 3 0]]\n",
      "[[ -5. -10.  -5.  -5.  -5.  -5.   0.]\n",
      " [ -5. -10.  -5.  -5.  -5. -10.   0.]\n",
      " [ -5. -10.  -5.  -5.  -5.  -5.   0.]\n",
      " [ -5. -10.  -5.  -5.  -5.  -5.   0.]]\n",
      "CPU times: user 810 ms, sys: 930 ms, total: 1.74 s\n",
      "Wall time: 760 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_, action_log, reward_log, _, _, _ = pool.interact(7)\n",
    "\n",
    "print(action_log)\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(64, replace=True)\n",
    "\n",
    "_, _, _, _, (qvalues_seq, old_qvalues_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      qvalues_target=old_qvalues_seq,\n",
    "                                                      gamma_or_gammas=0.99)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "grad = T.grad(loss, weights)\n",
    "updates = lasagne.updates.adam(grad,\n",
    "                               weights,\n",
    "                               learning_rate=0.0001)\n",
    "\n",
    "train_step = theano.function([], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 07:31:17,032] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 07:31:17,049] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-08-23 07:31:17,380] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.215.video000000.mp4\n",
      "[2017-08-23 07:31:18,315] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.215.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 2 timesteps with reward=91.0\n",
      "Episode finished after 70 timesteps with reward=-355.0\n",
      "Episode finished after 70 timesteps with reward=-355.0\n",
      "Episode finished after 70 timesteps with reward=-355.0\n",
      "Episode finished after 70 timesteps with reward=-355.0\n",
      "Episode finished after 2 timesteps with reward=91.0\n",
      "Episode finished after 2 timesteps with reward=91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 07:31:30,390] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.215.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-355.0\n",
      "Episode finished after 70 timesteps with reward=-355.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 07:31:35,981] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-355.0\n"
     ]
    }
   ],
   "source": [
    "action_layer.epsilon.set_value(0)\n",
    "untrained_reward = np.mean(pool.evaluate(save_path=\"./records\", record_video = True, n_games=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {0: untrained_reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have ```tqdm```, remove the first line and ```tqdm_notebook``` from second line\n",
    "\n",
    "Loop may take years to finish.\n",
    "\n",
    "You may consider interrupting early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=10\tepsilon=0.954\treward/step=-4.64773\n",
      "iter=20\tepsilon=0.910\treward/step=-3.83452\n",
      "iter=30\tepsilon=0.868\treward/step=-3.60081\n",
      "iter=40\tepsilon=0.828\treward/step=-3.82439\n",
      "iter=50\tepsilon=0.790\treward/step=-3.74069\n",
      "iter=60\tepsilon=0.754\treward/step=-3.80205\n",
      "iter=70\tepsilon=0.719\treward/step=-3.79261\n",
      "iter=80\tepsilon=0.687\treward/step=-3.95648\n",
      "iter=90\tepsilon=0.656\treward/step=-3.98819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6dadc4a7fac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtargetnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "\n",
    "for i in range(2000):  \n",
    "   \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH, append=True)\n",
    "    \n",
    "    loss = train_step()\n",
    "    \n",
    "    targetnet.load_weights(0.01)\n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = 0.05 + 0.95*np.exp(-epoch_counter/200.)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = pool.experience_replay.rewards.get_value().mean()\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\treward/step=%.5f\"%(epoch_counter,\n",
    "                                                         current_epsilon,\n",
    "                                                         pool_mean_reward))\n",
    "        \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time,rw = zip(*sorted(list(rewards.items()), key=lambda p: p[0]))\n",
    "plt.plot(time, map(np.mean,rw))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_layer.epsilon.set_value(0.001)\n",
    "rw = pool.evaluate(n_games=20, save_path=\"./records\", record_video=True)\n",
    "print(\"mean session score=%f.5\"%np.mean(rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "#save for display\n",
    "#save(action_layer,\"doombasic_dqn_2000.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "from random import choice\n",
    "#select the one you want\n",
    "videos = filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./records/\"))\n",
    "video_path = \"./records/\" + choice(videos)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework II\n",
    "Get it work. We want stable positive score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus assignment II\n",
    "* Better env\n",
    "  * Switch to DoomDefendCenter, DoomHealthGathering, DoomDeathmatch __or__ any atari game you want.\n",
    "  * Try to get `better_than_random` score on any of those environments __(2+++ pts)__\n",
    "  * Deploy a better network. Doom will likely need some recurrent netsle\n",
    "     * Find an arcitecture which maxsimizes score __(bonus points depend on your ```mean_reward```)__  \n",
    "     * Bonus can get large as you approach state-of-the-art\n",
    "     \n",
    "* Deploy a different RL algorithm\n",
    "  * Try at least two RL algorithms which had been learned during the course and try to compare them on ```mean_reward``` with similar training time (**plot** or **table** would be good idea) __(3 pts)__\n",
    "  * See the note in assignment 4.1 on how to train on-policy\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
