{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[this demo requires doom installed either from gym-pool or from [ppaquette's repo](https://github.com/ppaquette/gym-doom)]\n",
    "\n",
    "## Basic Doom demo\n",
    "\n",
    "This demo solves DoomBasic env with a simple q-learning with experience replay.\n",
    "\n",
    "Video observation forces you to use ```CNN```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu, floatX=float32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=device=cpu, floatX=float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:08,055] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from gym.wrappers import SkipWrapper\n",
    "from ppaquette_gym_doom.wrappers.action_space import ToDiscrete\n",
    "from agentnet.experiments.openai_gym.wrappers import PreprocessImage\n",
    "GAME_NAME = 'ppaquette/DoomBasic-v0'\n",
    "\n",
    "make_env = lambda: PreprocessImage(SkipWrapper(4)(ToDiscrete(\"minimal\")(gym.make(GAME_NAME))),\n",
    "                                   width=80, height=80, grayscale=True)\n",
    "\n",
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape: (1, 80, 80)\n",
      "n_action: 4\n"
     ]
    }
   ],
   "source": [
    "#global params.\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "SEQ_LENGTH = 10\n",
    "FRAME_NUMBER = 4\n",
    "print(\"observation shape:\", observation_shape)\n",
    "print(\"n_action:\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0 False\n",
      "(1, 80, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6e722b7d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e73239240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, r, done, _ = env.step(1)\n",
    "print(r, done)\n",
    "print(np.shape(obs))\n",
    "plt.imshow(obs[0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu, floatX=float32\n"
     ]
    }
   ],
   "source": [
    "#setup and import theano/lasagne. Prefer GPU\n",
    "%env THEANO_FLAGS=device=cpu, floatX=float32\n",
    "\n",
    "import theano, lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from agentnet.memory import WindowAugmentation, LSTMCell\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.target_network import TargetNetwork\n",
    "from agentnet.experiments.openai_gym.pool import EnvPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of observation layer (None, 1, 80, 80)\n",
      "shape of prev wnd (None, 4, 1, 80, 80)\n",
      "reshape to (-1, 4, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "#observation\n",
    "observation_layer = InputLayer((None,)+observation_shape,)\n",
    "print(\"shape of observation layer\", (None,)+observation_shape)\n",
    "\n",
    "#4-tick window over images\n",
    "prev_wnd = InputLayer((None, FRAME_NUMBER)+observation_shape)\n",
    "print(\"shape of prev wnd\", (None, FRAME_NUMBER)+observation_shape)\n",
    "new_wnd = WindowAugmentation(observation_layer, prev_wnd)\n",
    "        \n",
    "#reshape to (frame, h,w). If you don't use grayscale, 4 should become 12.\n",
    "wnd_reshape = reshape(new_wnd, (-1, FRAME_NUMBER*observation_shape[0])+observation_shape[1:])\n",
    "print(\"reshape to\", (-1, FRAME_NUMBER*observation_shape[0])+observation_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define your NN which probably solve the environment. \n",
    "\n",
    "#### Tips:\n",
    "1. Main component are likely to be ```Conv2D``` and ```Pool2DLayer```\n",
    "2. Batch normalization here might speeds up training but may get unstable if you use small experience replay buffer\n",
    "3. Last layers should be Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.nonlinearities import elu, tanh, softmax\n",
    "net = {}\n",
    "net['conv1_1'] = Conv2DLayer(wnd_reshape, 32, 3, pad=1)\n",
    "net['conv1_2'] = Conv2DLayer(net['conv1_1'], 32, 3, pad=1)\n",
    "net['pool1'] = Pool2DLayer(net['conv1_2'], 2)\n",
    "net['conv2_1'] = Conv2DLayer(net['pool1'], 32, 3, pad=1)\n",
    "net['conv2_2'] = Conv2DLayer(net['conv2_1'], 32, 3, pad=1)\n",
    "net['pool2'] = Pool2DLayer(net['conv2_2'], 2)\n",
    "dense = DenseLayer(net['pool2'], num_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#baseline for all qvalues\n",
    "qvalues_layer = DenseLayer(dense, num_units=n_actions, nonlinearity=None, name='qval')\n",
    "        \n",
    "#sample actions proportionally to policy_layer\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetnet = TargetNetwork(qvalues_layer)\n",
    "qvalues_old = targetnet.output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(qvalues_layer, qvalues_old),\n",
    "              agent_states={new_wnd: prev_wnd},\n",
    "              action_layers=action_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b, W, b, qval.W, qval.b]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer, trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:09,428] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 06:55:09,450] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 06:55:09,470] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 06:55:09,496] Making new env: ppaquette/DoomBasic-v0\n"
     ]
    }
   ],
   "source": [
    "pool = EnvPool(agent,\n",
    "               make_env, \n",
    "               n_games=4, #parallel games (only 1 so far)\n",
    "               max_size=1000) #experience replay pool holding last 1k sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 2 2 2 0]\n",
      " [2 2 2 2 1 2 3]\n",
      " [3 2 2 3 2 2 1]\n",
      " [2 2 2 2 2 2 0]]\n",
      "[[ -5.  -5.  -5.  -5.  -5.  -5.   0.]\n",
      " [ -5.  -5.  -5.  -5. -10.  -5.   0.]\n",
      " [ -5.  -5.  -5.  -5.  -5.  -5.   0.]\n",
      " [ -5.  -5.  -5.  -5.  -5.  -5.   0.]]\n",
      "CPU times: user 780 ms, sys: 1.09 s, total: 1.87 s\n",
      "Wall time: 791 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_, action_log, reward_log, _, _, _ = pool.interact(7)\n",
    "\n",
    "print(action_log)\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(64, replace=True)\n",
    "\n",
    "_, _, _, _, (qvalues_seq, old_qvalues_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      qvalues_target=old_qvalues_seq,\n",
    "                                                      gamma_or_gammas=0.99)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "grad = T.grad(loss, weights)\n",
    "updates = lasagne.updates.adam(grad,\n",
    "                               weights,\n",
    "                               learning_rate=0.0001)\n",
    "\n",
    "train_step = theano.function([], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:23,459] Making new env: ppaquette/DoomBasic-v0\n",
      "[2017-08-23 06:55:23,500] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-08-23 06:55:23,836] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.50.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-350.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:28,188] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.50.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:45,763] Starting new video recorder writing to /notebooks/week4/records/openaigym.video.0.50.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-350.0\n",
      "Episode finished after 70 timesteps with reward=-350.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 06:55:51,578] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 70 timesteps with reward=-350.0\n"
     ]
    }
   ],
   "source": [
    "action_layer.epsilon.set_value(0)\n",
    "untrained_reward = np.mean(pool.evaluate(save_path=\"./records\", record_video = True, n_games=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {0: untrained_reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have ```tqdm```, remove the first line and ```tqdm_notebook``` from second line\n",
    "\n",
    "Loop may take years to finish.\n",
    "\n",
    "You may consider interrupting early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=10\tepsilon=0.954\treward/step=-3.68864\n",
      "iter=20\tepsilon=0.910\treward/step=-3.43571\n",
      "iter=30\tepsilon=0.868\treward/step=-3.58387\n",
      "iter=40\tepsilon=0.828\treward/step=-3.85427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6dadc4a7fac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtargetnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "\n",
    "for i in range(2000):  \n",
    "   \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH, append=True)\n",
    "    \n",
    "    loss = train_step()\n",
    "    \n",
    "    targetnet.load_weights(0.01)\n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = 0.05 + 0.95*np.exp(-epoch_counter/200.)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = pool.experience_replay.rewards.get_value().mean()\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\treward/step=%.5f\"%(epoch_counter,\n",
    "                                                         current_epsilon,\n",
    "                                                         pool_mean_reward))\n",
    "        \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time,rw = zip(*sorted(list(rewards.items()), key=lambda p: p[0]))\n",
    "plt.plot(time, map(np.mean,rw))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_layer.epsilon.set_value(0.001)\n",
    "rw = pool.evaluate(n_games=20, save_path=\"./records\", record_video=True)\n",
    "print(\"mean session score=%f.5\"%np.mean(rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "#save for display\n",
    "#save(action_layer,\"doombasic_dqn_2000.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "from random import choice\n",
    "#select the one you want\n",
    "videos = filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./records/\"))\n",
    "video_path = \"./records/\" + choice(videos)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework II\n",
    "Get it work. We want stable positive score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus assignment II\n",
    "* Better env\n",
    "  * Switch to DoomDefendCenter, DoomHealthGathering, DoomDeathmatch __or__ any atari game you want.\n",
    "  * Try to get `better_than_random` score on any of those environments __(2+++ pts)__\n",
    "  * Deploy a better network. Doom will likely need some recurrent netsle\n",
    "     * Find an arcitecture which maxsimizes score __(bonus points depend on your ```mean_reward```)__  \n",
    "     * Bonus can get large as you approach state-of-the-art\n",
    "     \n",
    "* Deploy a different RL algorithm\n",
    "  * Try at least two RL algorithms which had been learned during the course and try to compare them on ```mean_reward``` with similar training time (**plot** or **table** would be good idea) __(3 pts)__\n",
    "  * See the note in assignment 4.1 on how to train on-policy\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
