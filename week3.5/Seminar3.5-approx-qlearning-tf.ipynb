{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to __TensorFlow__ almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is not str or len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-08 05:55:55,824] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 4 state: [-0.03117133 -0.02857845 -0.00350353 -0.04090892]\n",
      "n actions: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mdMuAlqVMGvCR16n8BrYh5yR9TTXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55083a17f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "s = env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "n_actions = env.action_space.n\n",
    "state_dim = np.product(env.observation_space.shape)\n",
    "print('state dim:', state_dim, 'state:', s)\n",
    "print('n actions:', n_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "assert tf.__version__ > \"1.0.0\", \"try pip install --upgrade tensorflow(-gpu)\"\n",
    "import tensorflow.contrib.layers as tflayers  # Let's make TF simple again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch_size_10_memory_capacity_10000_epsilon_0.1_gamma_0.9_lr_0.0001_reg_0.001_hidden_size_20\n"
     ]
    }
   ],
   "source": [
    "# handle parameters\n",
    "\n",
    "MEMORY_CAPACITY = 10000\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.9\n",
    "MINI_BATCH_SIZE = 10\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZATION = 0.001\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "ID = 0\n",
    "EPISODES = 2000\n",
    "STEPS = 500\n",
    "ENVIRONMENT = 'CartPole-v0'\n",
    "\n",
    "dqn_params = {'memory_capacity': MEMORY_CAPACITY, \n",
    "              'epsilon': EPSILON, \n",
    "              'gamma': GAMMA, \n",
    "              'mini_batch_size': MINI_BATCH_SIZE}\n",
    "\n",
    "nn_params = {'lr': LEARNING_RATE, \n",
    "             'reg': REGULARIZATION,\n",
    "             'hidden_size': HIDDEN_SIZE}\n",
    "\n",
    "run_id = '_'.join([str(k) + '_' + str(v) for k, v in dqn_params.items()] + \n",
    "                  [str(k) + '_' + str(v) for k, v in nn_params.items()])\n",
    "print(run_id)\n",
    "\n",
    "agent_params = {'episodes': EPISODES,\n",
    "                'steps': STEPS, \n",
    "                'environment':ENVIRONMENT, \n",
    "                'run_id': run_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \"\"\"\n",
    "    Neural Network model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, state_dim, params={}):\n",
    "        \"\"\"\n",
    "        Initialize the NN model with a set of parameters.\n",
    "        Args:\n",
    "            params: a dictionary containing values of the models' parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.lr = params['lr']\n",
    "        self.reg = params['reg']\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.session = self.create_model()\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        state_placeholder = tf.placeholder(tf.float32, shape=(None, self.state_dim))\n",
    "        value_placeholder = tf.placeholder(tf.float32, shape=(None,))\n",
    "        action_placeholder = tf.placeholder(tf.float32, shape=(None, self.num_actions))\n",
    "\n",
    "        return state_placeholder, value_placeholder, action_placeholder\n",
    "    \n",
    "    def nn(self, state_node):\n",
    "        with tf.variable_scope(\"Layer1\") as scope:\n",
    "            W1shape = [self.state_dim, self.hidden_size]\n",
    "            W1 = tf.get_variable(\"W1\", shape=W1shape,)\n",
    "            bshape = [1, self.hidden_size]\n",
    "            b1 = tf.get_variable(\"b1\", shape=bshape, initializer = tf.constant_initializer(0.0))\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        with tf.variable_scope(\"Layer2\") as scope:\n",
    "            W2shape = [self.hidden_size, self.hidden_size]\n",
    "            W2 = tf.get_variable(\"W2\", shape=W2shape,)\n",
    "            bshape = [1, self.hidden_size]\n",
    "            b2 = tf.get_variable(\"b2\", shape=bshape, initializer = tf.constant_initializer(0.0))\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        with tf.variable_scope(\"OutputLayer\") as scope:\n",
    "            Ushape = [self.hidden_size, self.num_actions]\n",
    "            U = tf.get_variable(\"U\", shape=Ushape)\n",
    "            b3shape = [1, self.num_actions]\n",
    "            b3 = tf.get_variable(\"b3\", shape=b3shape, initializer = tf.constant_initializer(0.0))\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        xW = tf.matmul(state_node, W1)\n",
    "        h = tf.tanh(tf.add(xW, b1))\n",
    "\n",
    "        xW = tf.matmul(h, W2)\n",
    "        h = tf.tanh(tf.add(xW, b2))\n",
    "\n",
    "        hU = tf.matmul(h, U)    \n",
    "        out = tf.add(hU, b3)\n",
    "\n",
    "        reg = self.reg * (tf.reduce_sum(tf.square(W1)) + tf.reduce_sum(tf.square(W2)) + tf.reduce_sum(tf.square(U)))\n",
    "        return out, reg\n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        The model definition.\n",
    "        \"\"\"\n",
    "        self.state_placeholder, self.value_placeholder, self.action_placeholder = self.add_placeholders()\n",
    "        outputs, reg = self.nn(self.state_placeholder)\n",
    "        self.predictions = outputs\n",
    "\n",
    "        self.q_vals = tf.reduce_sum(tf.multiply(self.predictions, self.action_placeholder), 1)\n",
    "\n",
    "        self.loss = tf.reduce_sum(tf.square(self.value_placeholder - self.q_vals)) + reg\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.lr)\n",
    "\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        session = tf.Session()\n",
    "        session.run(init)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def train_step(self, Xs, ys, actions):\n",
    "        \"\"\"\n",
    "        Updates the CNN model with a mini batch of training examples.\n",
    "        \"\"\"\n",
    "\n",
    "        loss, _, prediction_probs, q_values = self.session.run(\n",
    "          [self.loss, self.train_op, self.predictions, self.q_vals],\n",
    "          feed_dict = {self.state_placeholder: Xs,\n",
    "                      self.value_placeholder: ys,\n",
    "                      self.action_placeholder: actions\n",
    "                      })\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Predicts the rewards for an input observation state. \n",
    "        Args:\n",
    "        observation: a numpy array of a single observation state\n",
    "        \"\"\"\n",
    "\n",
    "        return self.session.run(self.predictions,\n",
    "                                feed_dict = {self.state_placeholder: state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, num_actions, state_dim, dqn_params, cnn_params):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = dqn_params['epsilon']\n",
    "        self.gamma = dqn_params['gamma']\n",
    "        self.mini_batch_size = dqn_params['mini_batch_size']\n",
    "\n",
    "        # memory \n",
    "        self.memory = collections.deque(maxlen=dqn_params['memory_capacity'])\n",
    "\n",
    "        # initialize network\n",
    "        self.model = NN(num_actions, state_dim, cnn_params)\n",
    "        print(\"model initialized\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects the next action to take based on the current state and learned Q.\n",
    "        Args:\n",
    "          observation: the current state\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # with epsilon probability select a random action\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # select the action a which maximizes the Q value\n",
    "            q_values = self.model.predict([state])\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_state(self, action, state, new_state, reward, done):\n",
    "        \"\"\"\n",
    "        Stores the most recent action in the replay memory.\n",
    "        Args: \n",
    "          action: the action taken \n",
    "          observation: the state before the action was taken\n",
    "          new_observation: the state after the action is taken\n",
    "          reward: the reward from the action\n",
    "          done: a boolean for when the episode has terminated \n",
    "        \"\"\"\n",
    "        transition = {'action': action,\n",
    "                      'state': state,\n",
    "                      'new_state': new_state,\n",
    "                      'reward': reward,\n",
    "                      'is_done': done}\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def get_random_mini_batch(self):\n",
    "        \"\"\"\n",
    "        Gets a random sample of transitions from the replay memory.\n",
    "        \"\"\"\n",
    "        rand_idxs = np.random.choice(range(len(self.memory)), self.mini_batch_size)\n",
    "        mini_batch = []\n",
    "        for idx in rand_idxs:\n",
    "          mini_batch.append(self.memory[idx])\n",
    "\n",
    "        return mini_batch\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Updates the model based on the mini batch\n",
    "        \"\"\"\n",
    "        if len(self.memory) > self.mini_batch_size:\n",
    "            mini_batch = self.get_random_mini_batch()\n",
    "\n",
    "            Xs = []\n",
    "            ys = []\n",
    "            actions = []\n",
    "\n",
    "            for sample in mini_batch:\n",
    "                y_j = sample['reward']\n",
    "\n",
    "                # for nonterminals, add gamma*max_a(Q(phi_{j+1})) term to y_j\n",
    "                if not sample['is_done']:\n",
    "                    new_state = sample['new_state']\n",
    "                    q_new_values = self.model.predict([new_state])\n",
    "                    max_action_value = np.max(q_new_values)\n",
    "                    y_j += self.gamma*max_action_value\n",
    "\n",
    "                action = np.zeros(self.num_actions)\n",
    "                action[sample['action']] = 1\n",
    "\n",
    "                state = sample['state']\n",
    "\n",
    "                Xs.append(state.copy())\n",
    "                ys.append(y_j)\n",
    "                actions.append(action.copy())\n",
    "\n",
    "            Xs = np.array(Xs)\n",
    "            ys = np.array(ys)\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            self.model.train_step(Xs, ys, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_dqn():\n",
    "    \n",
    "    env = gym.make(agent_params['environment'])\n",
    "    episodes = agent_params['episodes']\n",
    "    steps = agent_params['steps']\n",
    "    num_actions = env.action_space.n\n",
    "    state_dim = np.product(env.observation_space.shape)\n",
    "\n",
    "    # initialize dqn learning\n",
    "    dqn = DQN(num_actions, state_dim, dqn_params, nn_params)\n",
    "\n",
    "    last_100 = collections.deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        reward_sum = 0\n",
    "\n",
    "        if np.mean(last_100) > 200:\n",
    "            break\n",
    "\n",
    "        for t in range(steps):\n",
    "            env.render()\n",
    "            #print observation\n",
    "\n",
    "            # select action based on the model\n",
    "            action = dqn.select_action(state)\n",
    "            # execute actin in emulator\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            # update the state \n",
    "            dqn.update_state(action, state, new_state, reward, done)\n",
    "            state = new_state\n",
    "\n",
    "            # train the model\n",
    "            dqn.train_step()\n",
    "\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                print(\"Episode \", i_episode)\n",
    "                print(\"Finished after {} timesteps\".format(t+1))\n",
    "                print(\"Reward for this episode: \", reward_sum)\n",
    "                last_100.append(reward_sum)\n",
    "                print(\"Average reward for last 100 episodes: \", np.mean(last_100))\n",
    "                break\n",
    "\n",
    "    env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-08 05:55:58,894] Making new env: CartPole-v0\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      "Episode  0\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.0\n",
      "Episode  1\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.0\n",
      "Episode  2\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.3333333333\n",
      "Episode  3\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  4\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.6\n",
      "Episode  5\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.5\n",
      "Episode  6\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.28571428571\n",
      "Episode  7\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.375\n",
      "Episode  8\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.55555555556\n",
      "Episode  9\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.6\n",
      "Episode  10\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.63636363636\n",
      "Episode  11\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.5\n",
      "Episode  12\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.53846153846\n",
      "Episode  13\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.57142857143\n",
      "Episode  14\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.6\n",
      "Episode  15\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.625\n",
      "Episode  16\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.64705882353\n",
      "Episode  17\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.72222222222\n",
      "Episode  18\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.68421052632\n",
      "Episode  19\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  20\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.80952380952\n",
      "Episode  21\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.77272727273\n",
      "Episode  22\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.78260869565\n",
      "Episode  23\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  24\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.76\n",
      "Episode  25\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.76923076923\n",
      "Episode  26\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.74074074074\n",
      "Episode  27\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  28\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.72413793103\n",
      "Episode  29\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.7\n",
      "Episode  30\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  9.8064516129\n",
      "Episode  31\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.78125\n",
      "Episode  32\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.75757575758\n",
      "Episode  33\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.73529411765\n",
      "Episode  34\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.77142857143\n",
      "Episode  35\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.77777777778\n",
      "Episode  36\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.72972972973\n",
      "Episode  37\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.76315789474\n",
      "Episode  38\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.79487179487\n",
      "Episode  39\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.775\n",
      "Episode  40\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.75609756098\n",
      "Episode  41\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.7380952381\n",
      "Episode  42\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.74418604651\n",
      "Episode  43\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.72727272727\n",
      "Episode  44\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.68888888889\n",
      "Episode  45\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.67391304348\n",
      "Episode  46\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.63829787234\n",
      "Episode  47\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.64583333333\n",
      "Episode  48\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.63265306122\n",
      "Episode  49\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.6\n",
      "Episode  50\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.60784313725\n",
      "Episode  51\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.57692307692\n",
      "Episode  52\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.62264150943\n",
      "Episode  53\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.61111111111\n",
      "Episode  54\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.63636363636\n",
      "Episode  55\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.66071428571\n",
      "Episode  56\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.68421052632\n",
      "Episode  57\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.68965517241\n",
      "Episode  58\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.67796610169\n",
      "Episode  59\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.66666666667\n",
      "Episode  60\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.65573770492\n",
      "Episode  61\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.62903225806\n",
      "Episode  62\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.63492063492\n",
      "Episode  63\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.609375\n",
      "Episode  64\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.6\n",
      "Episode  65\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.60606060606\n",
      "Episode  66\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.61194029851\n",
      "Episode  67\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.63235294118\n",
      "Episode  68\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.63768115942\n",
      "Episode  69\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.62857142857\n",
      "Episode  70\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.61971830986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  71\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.61111111111\n",
      "Episode  72\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.61643835616\n",
      "Episode  73\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.59459459459\n",
      "Episode  74\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.61333333333\n",
      "Episode  75\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.61842105263\n",
      "Episode  76\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  9.67532467532\n",
      "Episode  77\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.69230769231\n",
      "Episode  78\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.67088607595\n",
      "Episode  79\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.6875\n",
      "Episode  80\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.71604938272\n",
      "Episode  81\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.71951219512\n",
      "Episode  82\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.72289156627\n",
      "Episode  83\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.71428571429\n",
      "Episode  84\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.71764705882\n",
      "Episode  85\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.72093023256\n",
      "Episode  86\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.71264367816\n",
      "Episode  87\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.69318181818\n",
      "Episode  88\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.69662921348\n",
      "Episode  89\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.68888888889\n",
      "Episode  90\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.69230769231\n",
      "Episode  91\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.6847826087\n",
      "Episode  92\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.68817204301\n",
      "Episode  93\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.6914893617\n",
      "Episode  94\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.69473684211\n",
      "Episode  95\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.6875\n",
      "Episode  96\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.69072164948\n",
      "Episode  97\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.70408163265\n",
      "Episode  98\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.69696969697\n",
      "Episode  99\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.7\n",
      "Episode  100\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.71\n",
      "Episode  101\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.69\n",
      "Episode  102\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.71\n",
      "Episode  103\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.74\n",
      "Episode  104\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.74\n",
      "Episode  105\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  106\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.77\n",
      "Episode  107\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.77\n",
      "Episode  108\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  109\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  110\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.73\n",
      "Episode  111\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  112\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.74\n",
      "Episode  113\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.72\n",
      "Episode  114\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.72\n",
      "Episode  115\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.72\n",
      "Episode  116\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.74\n",
      "Episode  117\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.74\n",
      "Episode  118\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.77\n",
      "Episode  119\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.75\n",
      "Episode  120\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.77\n",
      "Episode  121\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.78\n",
      "Episode  122\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.78\n",
      "Episode  123\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.78\n",
      "Episode  124\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.78\n",
      "Episode  125\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  126\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  127\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  128\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  129\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  130\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.79\n",
      "Episode  131\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  132\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.82\n",
      "Episode  133\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.82\n",
      "Episode  134\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  135\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  136\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  137\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  138\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.79\n",
      "Episode  139\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  140\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.8\n",
      "Episode  141\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.79\n",
      "Episode  142\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.78\n",
      "Episode  143\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  144\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.81\n",
      "Episode  145\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  9.85\n",
      "Episode  146\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.88\n",
      "Episode  147\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.9\n",
      "Episode  148\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.93\n",
      "Episode  149\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  150\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  151\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  152\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  153\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  154\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  155\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.94\n",
      "Episode  156\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.93\n",
      "Episode  157\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.94\n",
      "Episode  158\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.93\n",
      "Episode  159\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  160\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  161\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  162\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  163\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  164\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  165\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  166\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  167\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  168\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  169\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  170\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  171\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  172\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  173\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  174\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.08\n",
      "Episode  175\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  176\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  177\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  178\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  179\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  180\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  181\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  182\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  183\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  184\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  185\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  186\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  187\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  188\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  189\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  190\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  191\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.08\n",
      "Episode  192\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  193\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  194\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  195\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  196\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  197\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  198\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  199\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  200\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  201\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  202\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  203\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  204\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  205\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  206\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.95\n",
      "Episode  207\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.95\n",
      "Episode  208\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  209\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  210\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  211\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  212\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  213\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  214\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  215\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  216\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  217\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  218\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  219\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  220\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  221\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  222\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  223\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  224\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  225\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.96\n",
      "Episode  226\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  227\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  228\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  229\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  230\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  231\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  232\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  233\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  234\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  235\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  236\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  237\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  238\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  239\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  240\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  241\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  242\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.08\n",
      "Episode  243\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  244\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  245\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  246\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  247\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  248\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  249\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  250\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  251\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  9.97\n",
      "Episode  252\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.98\n",
      "Episode  253\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  254\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  9.99\n",
      "Episode  255\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  256\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  257\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  258\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  259\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  260\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  261\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  262\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  263\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  264\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  265\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  266\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.08\n",
      "Episode  267\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.1\n",
      "Episode  268\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.1\n",
      "Episode  269\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.1\n",
      "Episode  270\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.1\n",
      "Episode  271\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.07\n",
      "Episode  272\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  273\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  274\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  275\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.01\n",
      "Episode  276\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.0\n",
      "Episode  277\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  278\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  279\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.02\n",
      "Episode  280\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.03\n",
      "Episode  281\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  282\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.05\n",
      "Episode  283\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.04\n",
      "Episode  284\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  285\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.06\n",
      "Episode  286\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  287\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  288\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  289\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.11\n",
      "Episode  290\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.11\n",
      "Episode  291\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  292\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.11\n",
      "Episode  293\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.1\n",
      "Episode  294\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.09\n",
      "Episode  295\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.11\n",
      "Episode  296\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.13\n",
      "Episode  297\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.12\n",
      "Episode  298\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.13\n",
      "Episode  299\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.16\n",
      "Episode  300\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  301\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  302\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  303\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  304\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  305\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  306\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  307\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.2\n",
      "Episode  308\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  309\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  310\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.2\n",
      "Episode  311\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  312\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  313\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  314\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.17\n",
      "Episode  315\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  316\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.19\n",
      "Episode  317\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.2\n",
      "Episode  318\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  319\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.16\n",
      "Episode  320\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.18\n",
      "Episode  321\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  322\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.21\n",
      "Episode  323\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.22\n",
      "Episode  324\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.23\n",
      "Episode  325\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.26\n",
      "Episode  326\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.24\n",
      "Episode  327\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  328\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.29\n",
      "Episode  329\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  330\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.24\n",
      "Episode  331\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.25\n",
      "Episode  332\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  333\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  334\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  335\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.28\n",
      "Episode  336\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.27\n",
      "Episode  337\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.28\n",
      "Episode  338\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.31\n",
      "Episode  339\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  340\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  341\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  342\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  343\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.34\n",
      "Episode  344\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  345\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.34\n",
      "Episode  346\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.34\n",
      "Episode  347\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  348\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  349\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  350\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.37\n",
      "Episode  351\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  352\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  353\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  354\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.37\n",
      "Episode  355\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  356\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.34\n",
      "Episode  357\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  358\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  359\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  360\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  361\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  362\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  363\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  364\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  365\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  366\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.35\n",
      "Episode  367\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  368\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.32\n",
      "Episode  369\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.32\n",
      "Episode  370\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.33\n",
      "Episode  371\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.36\n",
      "Episode  372\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.37\n",
      "Episode  373\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  374\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.4\n",
      "Episode  375\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  376\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.44\n",
      "Episode  377\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  378\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.41\n",
      "Episode  379\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  380\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  381\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  382\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.44\n",
      "Episode  383\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.44\n",
      "Episode  384\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  385\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  386\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.4\n",
      "Episode  387\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.39\n",
      "Episode  388\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.37\n",
      "Episode  389\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.38\n",
      "Episode  390\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.41\n",
      "Episode  391\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  392\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.4\n",
      "Episode  393\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  394\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  395\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  396\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  397\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.45\n",
      "Episode  398\n",
      "Finished after 8 timesteps\n",
      "Reward for this episode:  8.0\n",
      "Average reward for last 100 episodes:  10.44\n",
      "Episode  399\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  400\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.43\n",
      "Episode  401\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.41\n",
      "Episode  402\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.42\n",
      "Episode  403\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.46\n",
      "Episode  404\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.47\n",
      "Episode  405\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.47\n",
      "Episode  406\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.48\n",
      "Episode  407\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.5\n",
      "Episode  408\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  409\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  410\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.55\n",
      "Episode  411\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.54\n",
      "Episode  412\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  413\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.51\n",
      "Episode  414\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.51\n",
      "Episode  415\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.51\n",
      "Episode  416\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.53\n",
      "Episode  417\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  418\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.53\n",
      "Episode  419\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.55\n",
      "Episode  420\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.55\n",
      "Episode  421\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.56\n",
      "Episode  422\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.55\n",
      "Episode  423\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.54\n",
      "Episode  424\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.53\n",
      "Episode  425\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  426\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.52\n",
      "Episode  427\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.5\n",
      "Episode  428\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.5\n",
      "Episode  429\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.51\n",
      "Episode  430\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.54\n",
      "Episode  431\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.53\n",
      "Episode  432\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  433\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.56\n",
      "Episode  434\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.54\n",
      "Episode  435\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.55\n",
      "Episode  436\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.56\n",
      "Episode  437\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  10.61\n",
      "Episode  438\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.6\n",
      "Episode  439\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.58\n",
      "Episode  440\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.59\n",
      "Episode  441\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.61\n",
      "Episode  442\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.64\n",
      "Episode  443\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.62\n",
      "Episode  444\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  10.67\n",
      "Episode  445\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.66\n",
      "Episode  446\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.68\n",
      "Episode  447\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.69\n",
      "Episode  448\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.69\n",
      "Episode  449\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.72\n",
      "Episode  450\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.75\n",
      "Episode  451\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.79\n",
      "Episode  452\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.8\n",
      "Episode  453\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.79\n",
      "Episode  454\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.81\n",
      "Episode  455\n",
      "Finished after 9 timesteps\n",
      "Reward for this episode:  9.0\n",
      "Average reward for last 100 episodes:  10.81\n",
      "Episode  456\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.82\n",
      "Episode  457\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.82\n",
      "Episode  458\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.82\n",
      "Episode  459\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.82\n",
      "Episode  460\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.85\n",
      "Episode  461\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.85\n",
      "Episode  462\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.85\n",
      "Episode  463\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.88\n",
      "Episode  464\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  10.92\n",
      "Episode  465\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  10.9\n",
      "Episode  466\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.9\n",
      "Episode  467\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.91\n",
      "Episode  468\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.93\n",
      "Episode  469\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.94\n",
      "Episode  470\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.95\n",
      "Episode  471\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.94\n",
      "Episode  472\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.93\n",
      "Episode  473\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  10.89\n",
      "Episode  474\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  10.93\n",
      "Episode  475\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  10.95\n",
      "Episode  476\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  10.98\n",
      "Episode  477\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.0\n",
      "Episode  478\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  11.04\n",
      "Episode  479\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.05\n",
      "Episode  480\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.09\n",
      "Episode  481\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.1\n",
      "Episode  482\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  11.13\n",
      "Episode  483\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.15\n",
      "Episode  484\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.16\n",
      "Episode  485\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.18\n",
      "Episode  486\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  11.23\n",
      "Episode  487\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  11.27\n",
      "Episode  488\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.3\n",
      "Episode  489\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.32\n",
      "Episode  490\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  11.34\n",
      "Episode  491\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.34\n",
      "Episode  492\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.38\n",
      "Episode  493\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.36\n",
      "Episode  494\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.4\n",
      "Episode  495\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.4\n",
      "Episode  496\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.41\n",
      "Episode  497\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.41\n",
      "Episode  498\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.44\n",
      "Episode  499\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.45\n",
      "Episode  500\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.46\n",
      "Episode  501\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.48\n",
      "Episode  502\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.47\n",
      "Episode  503\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.46\n",
      "Episode  504\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.49\n",
      "Episode  505\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  506\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.49\n",
      "Episode  507\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.49\n",
      "Episode  508\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.48\n",
      "Episode  509\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.49\n",
      "Episode  510\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  11.54\n",
      "Episode  511\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.56\n",
      "Episode  512\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.58\n",
      "Episode  513\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.62\n",
      "Episode  514\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.65\n",
      "Episode  515\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.67\n",
      "Episode  516\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.66\n",
      "Episode  517\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.68\n",
      "Episode  518\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.68\n",
      "Episode  519\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.7\n",
      "Episode  520\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.7\n",
      "Episode  521\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.68\n",
      "Episode  522\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  11.74\n",
      "Episode  523\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.74\n",
      "Episode  524\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  11.78\n",
      "Episode  525\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.78\n",
      "Episode  526\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.78\n",
      "Episode  527\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.79\n",
      "Episode  528\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  11.83\n",
      "Episode  529\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  11.89\n",
      "Episode  530\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.9\n",
      "Episode  531\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  11.96\n",
      "Episode  532\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  11.97\n",
      "Episode  533\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  11.95\n",
      "Episode  534\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.97\n",
      "Episode  535\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.96\n",
      "Episode  536\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  12.0\n",
      "Episode  537\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  11.99\n",
      "Episode  538\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.99\n",
      "Episode  539\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.04\n",
      "Episode  540\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  12.04\n",
      "Episode  541\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  12.07\n",
      "Episode  542\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  12.04\n",
      "Episode  543\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  12.06\n",
      "Episode  544\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  545\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  546\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  547\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  548\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  549\n",
      "Finished after 10 timesteps\n",
      "Reward for this episode:  10.0\n",
      "Average reward for last 100 episodes:  12.0\n",
      "Episode  550\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.99\n",
      "Episode  551\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.97\n",
      "Episode  552\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  11.97\n",
      "Episode  553\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.98\n",
      "Episode  554\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.97\n",
      "Episode  555\n",
      "Finished after 11 timesteps\n",
      "Reward for this episode:  11.0\n",
      "Average reward for last 100 episodes:  11.99\n",
      "Episode  556\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  12.01\n",
      "Episode  557\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.02\n",
      "Episode  558\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  12.06\n",
      "Episode  559\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.09\n",
      "Episode  560\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  12.1\n",
      "Episode  561\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.12\n",
      "Episode  562\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.16\n",
      "Episode  563\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.2\n",
      "Episode  564\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  12.23\n",
      "Episode  565\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.26\n",
      "Episode  566\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  12.31\n",
      "Episode  567\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.35\n",
      "Episode  568\n",
      "Finished after 18 timesteps\n",
      "Reward for this episode:  18.0\n",
      "Average reward for last 100 episodes:  12.42\n",
      "Episode  569\n",
      "Finished after 18 timesteps\n",
      "Reward for this episode:  18.0\n",
      "Average reward for last 100 episodes:  12.49\n",
      "Episode  570\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.5\n",
      "Episode  571\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  12.55\n",
      "Episode  572\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.59\n",
      "Episode  573\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  12.64\n",
      "Episode  574\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.66\n",
      "Episode  575\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.67\n",
      "Episode  576\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  577\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.71\n",
      "Episode  578\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  12.7\n",
      "Episode  579\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  12.76\n",
      "Episode  580\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.78\n",
      "Episode  581\n",
      "Finished after 18 timesteps\n",
      "Reward for this episode:  18.0\n",
      "Average reward for last 100 episodes:  12.84\n",
      "Episode  582\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  12.82\n",
      "Episode  583\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  12.85\n",
      "Episode  584\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  12.91\n",
      "Episode  585\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  12.96\n",
      "Episode  586\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  12.97\n",
      "Episode  587\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  13.0\n",
      "Episode  588\n",
      "Finished after 19 timesteps\n",
      "Reward for this episode:  19.0\n",
      "Average reward for last 100 episodes:  13.06\n",
      "Episode  589\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  13.08\n",
      "Episode  590\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  13.07\n",
      "Episode  591\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  13.1\n",
      "Episode  592\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  13.13\n",
      "Episode  593\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  13.17\n",
      "Episode  594\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  13.2\n",
      "Episode  595\n",
      "Finished after 18 timesteps\n",
      "Reward for this episode:  18.0\n",
      "Average reward for last 100 episodes:  13.28\n",
      "Episode  596\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  13.33\n",
      "Episode  597\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  13.38\n",
      "Episode  598\n",
      "Finished after 15 timesteps\n",
      "Reward for this episode:  15.0\n",
      "Average reward for last 100 episodes:  13.42\n",
      "Episode  599\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  13.48\n",
      "Episode  600\n",
      "Finished after 22 timesteps\n",
      "Reward for this episode:  22.0\n",
      "Average reward for last 100 episodes:  13.57\n",
      "Episode  601\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  13.59\n",
      "Episode  602\n",
      "Finished after 13 timesteps\n",
      "Reward for this episode:  13.0\n",
      "Average reward for last 100 episodes:  13.61\n",
      "Episode  603\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode:  20.0\n",
      "Average reward for last 100 episodes:  13.69\n",
      "Episode  604\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode:  20.0\n",
      "Average reward for last 100 episodes:  13.76\n",
      "Episode  605\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode:  20.0\n",
      "Average reward for last 100 episodes:  13.85\n",
      "Episode  606\n",
      "Finished after 21 timesteps\n",
      "Reward for this episode:  21.0\n",
      "Average reward for last 100 episodes:  13.96\n",
      "Episode  607\n",
      "Finished after 18 timesteps\n",
      "Reward for this episode:  18.0\n",
      "Average reward for last 100 episodes:  14.03\n",
      "Episode  608\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  14.07\n",
      "Episode  609\n",
      "Finished after 19 timesteps\n",
      "Reward for this episode:  19.0\n",
      "Average reward for last 100 episodes:  14.16\n",
      "Episode  610\n",
      "Finished after 19 timesteps\n",
      "Reward for this episode:  19.0\n",
      "Average reward for last 100 episodes:  14.18\n",
      "Episode  611\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode:  20.0\n",
      "Average reward for last 100 episodes:  14.26\n",
      "Episode  612\n",
      "Finished after 20 timesteps\n",
      "Reward for this episode:  20.0\n",
      "Average reward for last 100 episodes:  14.35\n",
      "Episode  613\n",
      "Finished after 14 timesteps\n",
      "Reward for this episode:  14.0\n",
      "Average reward for last 100 episodes:  14.36\n",
      "Episode  614\n",
      "Finished after 16 timesteps\n",
      "Reward for this episode:  16.0\n",
      "Average reward for last 100 episodes:  14.4\n",
      "Episode  615\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  14.44\n",
      "Episode  616\n",
      "Finished after 12 timesteps\n",
      "Reward for this episode:  12.0\n",
      "Average reward for last 100 episodes:  14.44\n",
      "Episode  617\n",
      "Finished after 17 timesteps\n",
      "Reward for this episode:  17.0\n",
      "Average reward for last 100 episodes:  14.5\n",
      "Episode  618\n",
      "Finished after 54 timesteps\n",
      "Reward for this episode:  54.0\n",
      "Average reward for last 100 episodes:  14.94\n",
      "Episode  619\n",
      "Finished after 24 timesteps\n",
      "Reward for this episode:  24.0\n",
      "Average reward for last 100 episodes:  15.05\n",
      "Episode  620\n",
      "Finished after 51 timesteps\n",
      "Reward for this episode:  51.0\n",
      "Average reward for last 100 episodes:  15.46\n",
      "Episode  621\n",
      "Finished after 45 timesteps\n",
      "Reward for this episode:  45.0\n",
      "Average reward for last 100 episodes:  15.8\n",
      "Episode  622\n",
      "Finished after 55 timesteps\n",
      "Reward for this episode:  55.0\n",
      "Average reward for last 100 episodes:  16.19\n",
      "Episode  623\n",
      "Finished after 53 timesteps\n",
      "Reward for this episode:  53.0\n",
      "Average reward for last 100 episodes:  16.62\n",
      "Episode  624\n",
      "Finished after 50 timesteps\n",
      "Reward for this episode:  50.0\n",
      "Average reward for last 100 episodes:  16.98\n",
      "Episode  625\n",
      "Finished after 56 timesteps\n",
      "Reward for this episode:  56.0\n",
      "Average reward for last 100 episodes:  17.42\n",
      "Episode  626\n",
      "Finished after 200 timesteps\n",
      "Reward for this episode:  200.0\n",
      "Average reward for last 100 episodes:  19.31\n",
      "Episode  627\n",
      "Finished after 119 timesteps\n",
      "Reward for this episode:  119.0\n",
      "Average reward for last 100 episodes:  20.37\n",
      "Episode  628\n",
      "Finished after 140 timesteps\n",
      "Reward for this episode:  140.0\n",
      "Average reward for last 100 episodes:  21.62\n",
      "Episode  629\n",
      "Finished after 75 timesteps\n",
      "Reward for this episode:  75.0\n",
      "Average reward for last 100 episodes:  22.21\n",
      "Episode  630\n",
      "Finished after 82 timesteps\n",
      "Reward for this episode:  82.0\n",
      "Average reward for last 100 episodes:  22.9\n",
      "Episode  631\n",
      "Finished after 72 timesteps\n",
      "Reward for this episode:  72.0\n",
      "Average reward for last 100 episodes:  23.47\n",
      "Episode  632\n",
      "Finished after 67 timesteps\n",
      "Reward for this episode:  67.0\n",
      "Average reward for last 100 episodes:  24.01\n",
      "Episode  633\n",
      "Finished after 67 timesteps\n",
      "Reward for this episode:  67.0\n",
      "Average reward for last 100 episodes:  24.58\n",
      "Episode  634\n",
      "Finished after 24 timesteps\n",
      "Reward for this episode:  24.0\n",
      "Average reward for last 100 episodes:  24.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-56c314087e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a90f7968cfc6>\u001b[0m in \u001b[0;36mrun_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# select action based on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# execute actin in emulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d3696033a197>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# select the action a which maximizes the Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8c975bbfcdea>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         return self.session.run(self.predictions,\n\u001b[0;32m--> 105\u001b[0;31m                                 feed_dict = {self.state_placeholder: state})\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "\n",
    "    def __init__(self, n_actions, state_dim, epsilon, alpha, gamma, sess=None):\n",
    "        self.graph = tf.Graph()\n",
    "        self.n_actions = n_actions\n",
    "        self.state_dim = np.product(state_dim)\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.h = 10\n",
    "        \n",
    "        # construct a network\n",
    "        with self.graph.as_default():\n",
    "            self.q_goal = tf.placeholder('float32')\n",
    "            self.s = tf.placeholder('float32', [self.state_dim])\n",
    "            self.a = tf.placeholder('uint8')\n",
    "            a_hot = tf.reshape(tf.one_hot(indices=self.a, depth=n_actions, dtype='float32'), [1, -1])\n",
    "            \n",
    "            Ws = self.weight_variable([self.state_dim, self.h])\n",
    "            Wa = self.weight_variable([n_actions, self.h])\n",
    "            b = self.bias_variable([self.h])\n",
    "            \n",
    "            hidden = tf.sigmoid(tf.matmul(tf.reshape(self.s, [1,-1]), Ws) + \n",
    "                                    tf.matmul(a_hot, Wa) + b)\n",
    "            \n",
    "            W2 = self.weight_variable([self.h, 1])\n",
    "            b2 = self.bias_variable([1])\n",
    "            \n",
    "            self.score = tf.sigmoid(tf.matmul(hidden, W2) + b2) \n",
    "            \n",
    "            loss = (self.q_goal - self.score)**2\n",
    "            self.fit = tf.train.AdamOptimizer(learning_rate=self.alpha).minimize(loss)\n",
    "            self.session = sess or tf.Session()\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # tensorboard\n",
    "            writer = tf.summary.FileWriter(\"/tmp/tboard\", graph=tf.get_default_graph())\n",
    "            writer.close()\n",
    "    \n",
    "    def get_Q(self, state, action):\n",
    "        \n",
    "        feed = {self.s:state, self.a:action}\n",
    "        sess = self.session\n",
    "        q_value = sess.run(self.score, feed_dict=feed)\n",
    "        return q_value\n",
    "\n",
    "    def get_Q_state_only(self, state):\n",
    "        \n",
    "        return self.get_Q(state, self.get_best_action(state))\n",
    "        \n",
    "    def get_best_action(self, state):\n",
    "        \n",
    "        q_values = [self.get_Q(state, act) for act in range(n_actions)]\n",
    "        best = np.argmax(q_values)\n",
    "        \n",
    "        return best\n",
    "        \n",
    "    def get_action_epsilon_greedy(self, state):\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            act = np.random.randint(n_actions)\n",
    "        else:\n",
    "            act = self.get_best_action(state)\n",
    "            \n",
    "        return act\n",
    "        \n",
    "    def update(self, s1, a, r, s2):\n",
    "        \n",
    "        sess = self.session\n",
    "        goal = r + self.gamma*self.get_Q_state_only(s2)\n",
    "\n",
    "        feed = {self.q_goal:goal,\n",
    "                self.s:s1,\n",
    "                self.a:a}\n",
    "\n",
    "        sess.run(self.fit, feed_dict=feed)\n",
    "            \n",
    "    @staticmethod    \n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.02)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.05, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_epsilon = 0.5\n",
    "n_epochs = 1000\n",
    "alpha = 0.0001\n",
    "gamma = 0.95\n",
    "\n",
    "agent = DeepQAgent(n_actions, state_dim, initial_epsilon, alpha, gamma)\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        \n",
    "        a = agent.get_action_epsilon_greedy(s)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        agent.update(s, a, r, new_s)\n",
    "\n",
    "        total_reward += r\n",
    "        s = new_s\n",
    "\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward, 0, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "tr = trange(\n",
    "    n_epochs,\n",
    "    desc=\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tsteps = {:.3f}\".format(0.0, 0.0, 0.0),\n",
    "    leave=True)\n",
    "\n",
    "\n",
    "for i in tr:\n",
    "    \n",
    "    sessions = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    session_rewards, _, session_steps = map(np.array, zip(*sessions))\n",
    "    \n",
    "    agent.epsilon = max(agent.epsilon*0.99, 0.1)\n",
    "    \n",
    "    tr.set_description(\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tsteps = {:.3f}\".format(\n",
    "        np.mean(session_rewards), agent.epsilon, np.mean(session_steps)))\n",
    "\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert initial_epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.epsilon=0 \n",
    "#Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Homework\n",
    "\n",
    "Two paths lie ahead of you, and which one to take is a rightfull choice of yours.\n",
    "\n",
    "* __[recommended]__ Go deeper. Return to seminar1 and get 99% accuracy on MNIST\n",
    "* __[alternative]__ Try approximate expected-value SARSA and other algorithms and compare it with q-learning \n",
    "  * +3 points for EV-SARSA and comparison to Q-learning\n",
    "  * +2 per additional algorithm\n",
    "* __[alternative hard]__ Pick ```<your favourite env>``` and solve it, using NN.\n",
    " * LunarLander, MountainCar or Breakout (from week1 bonus)\n",
    " * LunarLander should get at least +100\n",
    " * MountainCar should get at least -200\n",
    " * You will need to somehow stabilize learning\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.choice(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"{'epsilon': 0.1, 'gamma': 0.9, 'mini_batch_size': 10, 'memory_capacity': 10000}{'hidden_size': 20, 'lr': 0.0001, 'reg': 0.001}-id-0\"\n",
    "s.split([':', '\"', \"'\", ','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
