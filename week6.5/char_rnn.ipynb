{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "#thanks @keskarnitish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate names\n",
    "* Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train NN instead.\n",
    "* Dataset contains ~8k human names from different cultures[in latin transcript]\n",
    "* Objective (toy problem): learn a generative model over names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      " Abagael\n",
      " Claresta\n",
      " Glory\n",
      " Liliane\n",
      " Prissie\n",
      " Geeta\n",
      " Giovanne\n",
      " Piggy\n"
     ]
    }
   ],
   "source": [
    "print ('n samples = ',len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  55\n",
      "['P', 'n', 'C', 'l', 'h', 's', 'm', 'Z', 'b', 'y', 'o', 'M', 'B', 'N', 'Y', 'j', 'R', 'r', 'J', '-', 'a', 'G', 'T', 'H', 'k', 'K', 'O', 'd', 't', 'W', 'p', 'U', 'X', 'z', 'u', 'v', 'w', 'f', 'D', 'L', 'I', 'e', 'F', 'Q', 'i', 'x', 'E', \"'\", 'g', 'A', 'S', ' ', 'q', 'c', 'V']\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "# <all unique characters in the dataset>\n",
    "from functools import reduce\n",
    "tokens = reduce(lambda x, y: x.union(y), [set()]+names)\n",
    "tokens = list(tokens)\n",
    "print('n_tokens = ', len(tokens))\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "token_to_id = {t:i for i, t in enumerate(tokens) }\n",
    "\n",
    "#!id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "id_to_token = {i:t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCQ1zBCQpIb9P1cUm6p4rRd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f029478ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate names longer than MAX_LEN characters. \n",
    "MAX_LEN = 20\n",
    "#you will likely need to change this for any dataset different from \"names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get, name)), names))\n",
    "\n",
    "\n",
    "#crop long names and pad short ones\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len, names_ix))) == 1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51 49  8 20 48 20 41  3 51 51 51 51 51 51 51 51 51 51 51 51]\n",
      " [51 49  8 20 48 20 44  3 51 51 51 51 51 51 51 51 51 51 51 51]\n",
      " [51 49  8  8 41 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51]]\n"
     ]
    }
   ],
   "source": [
    "print(names_ix[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequencea', 'int32')\n",
    "target_values = T.matrix('actual next token', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN\n",
    "\n",
    "You will be building a model that takes token sequence and predicts next token\n",
    "\n",
    "\n",
    "* iput sequence\n",
    "* one-hot / embedding\n",
    "* recurrent layer(s)\n",
    "* otput layer(s) that predict output probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer, EmbeddingLayer\n",
    "from lasagne.layers import RecurrentLayer, LSTMLayer, GRULayer, CustomRecurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 8\n",
    "HIDDEN_SIZE = 24\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None), input_var=input_sequence)\n",
    "\n",
    "#!<Your neural network>\n",
    "# <embedding layer or one-hot encoding>\n",
    "l_emb = EmbeddingLayer(l_in,\n",
    "                       input_size=len(tokens),\n",
    "                       output_size=EMBEDDING_SIZE)\n",
    "\n",
    "# <some recurrent layer(or several such layers)>\n",
    "l_rnn = LSTMLayer(l_emb,\n",
    "                  num_units=HIDDEN_SIZE,\n",
    "                  grad_clipping=5.)\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "l_rnn_flat = lasagne.layers.reshape(l_rnn, (-1, l_rnn.output_shape[-1]))\n",
    "\n",
    "# <last dense layer (or several layers), returning probabilities for all possible next tokens>\n",
    "l_out = DenseLayer(l_rnn_flat,\n",
    "                   num_units=len(tokens),\n",
    "                   nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne.regularization import regularize_layer_params, l2\n",
    "\n",
    "predicted_probabilities_flat = network_output\n",
    "correct_answers_flat = lasagne.utils.one_hot(target_values.ravel(), len(tokens))\n",
    "\n",
    "# <loss function - a simple categorical crossentropy will do, maybe add some regularizer>\n",
    "reg = regularize_layer_params([l_rnn, l_out], l2) * 1e-4\n",
    "loss = T.mean(T.nnet.categorical_crossentropy(predicted_probabilities_flat,\n",
    "                                              correct_answers_flat)) + reg\n",
    "\n",
    "# <your favorite optimizer>\n",
    "updates = lasagne.updates.adam(loss, weights, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "train = theano.function([input_sequence, target_values],\n",
    "                        loss,\n",
    "                        updates=updates,\n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation\n",
    "\n",
    "Simple: \n",
    "* get initial context(seed), \n",
    "* predict next token probabilities,\n",
    "* sample next token, \n",
    "* add it to the context\n",
    "* repeat from step 2\n",
    "\n",
    "You'll get a more detailed info on how it works in the homework section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "# reshape back into original shape\n",
    "next_word_probas = network_output.reshape((input_sequence.shape[0], \n",
    "                                           input_sequence.shape[1], len(tokens)))\n",
    "\n",
    "# predictions for next tokens (after sequence end)\n",
    "last_word_probas = next_word_probas[:, -1]\n",
    "probs = theano.function([input_sequence], last_word_probas, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sample(seed_phrase=None, N=MAX_LEN, t=1, n_snippets=1):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        seed_phrase=start_token\n",
    "    if len(seed_phrase) > MAX_LEN:\n",
    "        seed_phrase = seed_phrase[-MAX_LEN:]\n",
    "    assert type(seed_phrase) is str\n",
    "\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        sample_ix = []\n",
    "        x = list(map(lambda c: token_to_id.get(c,0), seed_phrase))\n",
    "        x = np.array([x])\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            p = probs(x).ravel()\n",
    "            p = p**t / np.sum(p**t)\n",
    "            ix = np.random.choice(np.arange(len(tokens)),p=p)\n",
    "            sample_ix.append(ix)\n",
    "\n",
    "            x = np.hstack((x[-MAX_LEN+1:],[[ix]]))\n",
    "\n",
    "        random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "        snippets.append(random_snippet.strip())\n",
    "        \n",
    "    print(\"----\\n %s \\n----\" % '; '.join(snippets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Here you can tweak parameters or insert your generation function\n",
    "\n",
    "\n",
    "__Once something word-like starts generating, try increasing seq_length__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0, len(data), size=batch_size)]\n",
    "    \n",
    "    return rows[:,:-1], rows[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Generated names\n",
      "----\n",
      " gCErGIrUyMWbEZcbjCp; ZrCfmaSqoxzM-LmaVtC'; AmKfcMlWEVVMPY UXYHU; oTAXHWEdxUWhSHQbAloX; pPNHDxRDFiwikdgkXIdd; xlImQOozTFasL'IusGVb; XYnX MyIAkXHsVnKfVvT; IJakfTkdfVgwQNgOaBrA; dwl-gqLtTnNwhUj'C xa; aUKfRpHCvUlxateM'mNj \n",
      "----\n",
      "Epoch 0 average loss = 1.5571671979710906\n",
      "Generated names\n",
      "----\n",
      " lihNSi; wiX  a; tyTr oa; hak  t; OL  o; gPadnie; wrh i  d; Cari e; hninrlli; agUoT   i  e \n",
      "----\n",
      "Epoch 1 average loss = 1.1446004772359593\n",
      "Generated names\n",
      "----\n",
      " Franee      n; lrseAp; dDh; tJon    i; lae n; bnZ; FR t; trreeQ  in; yOlere; LvWeia \n",
      "----\n",
      "Epoch 2 average loss = 1.0775628580632548\n",
      "Generated names\n",
      "----\n",
      " edyaia; watrleit; Ddina; Liiiie; ranl; CXirtrn; Unioi; mcid; nrc               s; OKieis \n",
      "----\n",
      "Epoch 3 average loss = 1.0021226760310546\n",
      "Generated names\n",
      "----\n",
      " Aeessintiya; Jla-la; Eehina; Ejga; Kzblnon; Splnfe; Aiiauca; Gaaenra; S eorusi; Crnlne \n",
      "----\n",
      "Epoch 4 average loss = 0.9750524173260583\n",
      "Generated names\n",
      "----\n",
      " Wyrilal; Harinlm; Meste ny; Sayrire; Stkr; Cgrred; Cayioy; zireuo; rbrZlhe; Rasntn \n",
      "----\n",
      "Epoch 5 average loss = 0.9395562467816247\n",
      "Generated names\n",
      "----\n",
      " Lalany; Dekthe; Lorasha; nuinyn; Eada; Xiy; diXviite; Sanare; JaNes; Lromri \n",
      "----\n",
      "Epoch 6 average loss = 0.9273028651851548\n",
      "Generated names\n",
      "----\n",
      " Sola; bhine; Enlelota; Jarminy; Simfriie; Radnany; Kiga; Suly; Dernhfsmi; Palgy \n",
      "----\n",
      "Epoch 7 average loss = 0.9192650728829511\n",
      "Generated names\n",
      "----\n",
      " EOle; Mhria; Binse; APu; Leea; Jitco; Ranralde; Noblsae; Ladenise; Sun \n",
      "----\n",
      "Epoch 8 average loss = 0.9115932758470778\n",
      "Generated names\n",
      "----\n",
      " Rar; Daris; Muronida; Jisl; Choefsota; Pildes; Aylte; Buletilala; Arras; Kaevly \n",
      "----\n",
      "Epoch 9 average loss = 0.9011754528763417\n",
      "Generated names\n",
      "----\n",
      " Snac; Longilols; Cirrin; Aharia; Jollis; Tole; Jasle; Burege; Fis; Elelron \n",
      "----\n",
      "Epoch 10 average loss = 0.8933406091690831\n",
      "Generated names\n",
      "----\n",
      " Beg; Kuyltal; Khidentee; Cnaciie; Elogtetee; Arlilas; Remi; Edeirtsa; Tednten; Busrplie \n",
      "----\n",
      "Epoch 11 average loss = 0.8931759354370374\n",
      "Generated names\n",
      "----\n",
      " Jand; Duy; Mnarnely; Jiewy; Mayrey; Tahaflid; Moliie; Lelie; Sbiset; Wetreli \n",
      "----\n",
      "Epoch 12 average loss = 0.8784198836193371\n",
      "Generated names\n",
      "----\n",
      " Larli; Ecegy; Lenyde; Botle; Rolly; Cate; Roune; Jeugtle; Jouarlie; Wili \n",
      "----\n",
      "Epoch 13 average loss = 0.8813139164995497\n",
      "Generated names\n",
      "----\n",
      " Einet; Ninie; Fidyn; Jeuonsil; Mudidte; Rics; Filgekpin; Laifs; Elter; Lebec \n",
      "----\n",
      "Epoch 14 average loss = 0.876141771289246\n",
      "Generated names\n",
      "----\n",
      " Daninta; Anpqigon; Srordas; Hujetle; Euday; Ttfyn; Egsia; NynQ; Jasahe; Merie \n",
      "----\n",
      "Epoch 15 average loss = 0.8727827540047749\n",
      "Generated names\n",
      "----\n",
      " Josa; Mistath; Wyle; Milia; Marna; Zowasgin; Aina; Mory; Lerny; Gecnia \n",
      "----\n",
      "Epoch 16 average loss = 0.8723569093514696\n",
      "Generated names\n",
      "----\n",
      " Lerly; Acre; Cesher; Diille; Courenni; Mela; Fimhan; Tustie; Roamol; Daby \n",
      "----\n",
      "Epoch 17 average loss = 0.866726822648984\n",
      "Generated names\n",
      "----\n",
      " Craetata; Balel; Cannbik; Sgimia; Almina; Jeda; Eynleet; Minety; Jobatal; Leriry \n",
      "----\n",
      "Epoch 18 average loss = 0.8669784359779054\n",
      "Generated names\n",
      "----\n",
      " Duudy; SauDad; Chamre; Ital; Cililit; Rikon; Caingile; Alerlen; Thende; Mumishy \n",
      "----\n",
      "Epoch 19 average loss = 0.8554540743111979\n",
      "Generated names\n",
      "----\n",
      " Oindi; Aryniong; nuelhe; Waolad; Amonia; Radee; Woprik; Heldieh; Teud; Rerie \n",
      "----\n",
      "Epoch 20 average loss = 0.8609506412780458\n",
      "Generated names\n",
      "----\n",
      " Heyndins; Godietta; Vorea; Carrfeen; Foris n; Gart; Sfacid; Roellace; Rania; Galcie \n",
      "----\n",
      "Epoch 21 average loss = 0.8546079845721395\n",
      "Generated names\n",
      "----\n",
      " Cumlk; Melxey; Sasce; Krisa; Vanelk; Pall; Elier; Lagrip; Laibe; Bataran \n",
      "----\n",
      "Epoch 22 average loss = 0.8539804175248706\n",
      "Generated names\n",
      "----\n",
      " Noyne; Edtelfe; Dagit; Berlua; Telli; Golene; Attabean; uadarine; Furvel; Warllm \n",
      "----\n",
      "Epoch 23 average loss = 0.8494050952215165\n",
      "Generated names\n",
      "----\n",
      " Eumend; Hilila; Marsis; Vero; Ertir; Loreptant; Ricondy; ZonN; Morly; Cebly \n",
      "----\n",
      "Epoch 24 average loss = 0.8482313737723506\n",
      "Generated names\n",
      "----\n",
      " Pamria; mikora; Alnosa; Lisget; Ansy; Nat; Teumi; Pollie; Wiesda; Bortel \n",
      "----\n",
      "Epoch 25 average loss = 0.8491304492896856\n",
      "Generated names\n",
      "----\n",
      " Qili; Mittet; Ribitar; Kerilu; Fertta; Kethin; Wherine; Rone; Salmy; Aloin \n",
      "----\n",
      "Epoch 26 average loss = 0.8489755510133421\n",
      "Generated names\n",
      "----\n",
      " NiDerdiqkee; Daniut; RoQsela; Cawcay; EGhad; Pbra; Jolie; Char; Zozderina; Debilit \n",
      "----\n",
      "Epoch 27 average loss = 0.8480500599996192\n",
      "Generated names\n",
      "----\n",
      " HhriramiaD; Ellalli; Mulnito; Sorin; Laqionele; Vahile; Lcicu; Watsari; Gleoria; Shigsen \n",
      "----\n",
      "Epoch 28 average loss = 0.8414529069718414\n",
      "Generated names\n",
      "----\n",
      " DaFlwe; Nacinn; Hitiann-utve; Hanasne; Emaria; Dlogsere; Andy; Shiena; Jatpdia; Aleri \n",
      "----\n",
      "Epoch 29 average loss = 0.8450383885068046\n",
      "Generated names\n",
      "----\n",
      " Brornke; Joens; CZahary; Cerigie; Cutsich; Adengia; Amria; Dyne; Miina; Gickid \n",
      "----\n",
      "Epoch 30 average loss = 0.8397683542813615\n",
      "Generated names\n",
      "----\n",
      " Gehrie; Robisa; Egchetar; Agdanne; Cunsta; Laro; IZe; Asel; Ditisha; Kinte \n",
      "----\n",
      "Epoch 31 average loss = 0.8422933799005489\n",
      "Generated names\n",
      "----\n",
      " Hunso; Auga; -Marrvey; Avyne; Joerallo; La; Gineline; Abie; Kamad; Laittoen \n",
      "----\n",
      "Epoch 32 average loss = 0.8356651102687751\n",
      "Generated names\n",
      "----\n",
      " Debyla; Jee; Elaca; Jonne; Guly; Sbine; Merdry; Latin; Ernipta; Banman \n",
      "----\n",
      "Epoch 33 average loss = 0.8414329123194159\n",
      "Generated names\n",
      "----\n",
      " Ansheris; Iunis; Nolitie; Ashane; Jabinnee; Iluce; Chaia; Anna     m; Ranipa; Ellic \n",
      "----\n",
      "Epoch 34 average loss = 0.835544870875505\n",
      "Generated names\n",
      "----\n",
      " Nevire; Garina; Almena; Roly; Alet; Ttalin; Lanein; Lerrann; Nathardu; Daimelle \n",
      "----\n",
      "Epoch 35 average loss = 0.834811643580565\n",
      "Generated names\n",
      "----\n",
      " Bayaonne; Isune; Lawita; Tary; duvrian; Belin; Flesmrith; Detharpair; Feyvril; Homin \n",
      "----\n",
      "Epoch 36 average loss = 0.8331057969816212\n",
      "Generated names\n",
      "----\n",
      " Meydi; Jo; Esgie; Kertina; Ienall; Mellinea; Jomesa; Robay; Mullbiete; Aminan \n",
      "----\n",
      "Epoch 37 average loss = 0.8343590655383228\n",
      "Generated names\n",
      "----\n",
      " Bunbia; Tavmee; Caduthi; Honlellaz; Racile; Tymber; Varrettond; Cezdy; YaAnneld; Datuh \n",
      "----\n",
      "Epoch 38 average loss = 0.8324456642287201\n",
      "Generated names\n",
      "----\n",
      " Anncie; Melisha; Pavwel; Yisnayn; Holdy; Chelbit; Cunpiy; Rabysa; Bwillis; Rondy \n",
      "----\n",
      "Epoch 39 average loss = 0.8292848574030578\n",
      "Generated names\n",
      "----\n",
      " Posaf; Dammun; Ceith; Koura; Lommi; Ghyre; Edsyn; Naner; Garincta; ALaeret \n",
      "----\n",
      "Epoch 40 average loss = 0.8300526783116055\n",
      "Generated names\n",
      "----\n",
      " Kitci; Ulandine; Kelca; Kacind; Maisharis; Jerleille; Hoipee; Geteie; Bagle; Cinila \n",
      "----\n",
      "Epoch 41 average loss = 0.8325226879209159\n",
      "Generated names\n",
      "----\n",
      " Radli; Tiyna; Anneamd; Cgevara; Sharn; Farellee; Jeann; Farsall; Denire; Cizon \n",
      "----\n",
      "Epoch 42 average loss = 0.8238047113375131\n",
      "Generated names\n",
      "----\n",
      " Narnerie; Alfie; Killfen; Ambink; Bevrid; Tonny; Kricke; Dary; Gysanis; Hany \n",
      "----\n",
      "Epoch 43 average loss = 0.8320147830220989\n",
      "Generated names\n",
      "----\n",
      " Giody; Leynta; Dauhdiss; Duttia; Annell; Wold; Jafdie; Hwiatta; Wyonna; Toshanguns \n",
      "----\n",
      "Epoch 44 average loss = 0.8313685256314118\n",
      "Generated names\n",
      "----\n",
      " Nimphian; Diseleno; Tprilerth; Vefride; Eda; Tolae; Luagsann; Torli; Joren; Velline \n",
      "----\n",
      "Epoch 45 average loss = 0.8262413165339154\n",
      "Generated names\n",
      "----\n",
      " Tennaline; Aroren; LeBby; Muavyna; Ularila; Jau; Woiderie; Dystie; darry; Refa \n",
      "----\n",
      "Epoch 46 average loss = 0.8265146622980032\n",
      "Generated names\n",
      "----\n",
      " Aloralelo; Mardien; Mwarsine; Mad; Jena; Crerprie; Nebey; Iban; Shayna; Alin \n",
      "----\n",
      "Epoch 47 average loss = 0.8222835407786435\n",
      "Generated names\n",
      "----\n",
      " Crordine; Otsa; Zeever; Loanne; Ann  Re; Renne; Therbadl; JerJen; Fwande; Erla \n",
      "----\n",
      "Epoch 48 average loss = 0.8266466428463756\n",
      "Generated names\n",
      "----\n",
      " Aralann; Make; Josean; Forbis; Luesa; Kuszo; Gemsel; Baivarie; Lyfgeil; Hatlana \n",
      "----\n",
      "Epoch 49 average loss = 0.8268791854753934\n",
      "Generated names\n",
      "----\n",
      " Wilna; Torel; Ebrena; uim; Valuisa; Marfis; Sheiina; Ferry; Biclde; Mernon \n",
      "----\n",
      "Epoch 50 average loss = 0.8289510962595527\n",
      "Generated names\n",
      "----\n",
      " Fuvily; Jopsol; Shemir; Cristie; Newiee; Carie; Frillfeo; Riofri; Lannena; Fasia \n",
      "----\n",
      "Epoch 51 average loss = 0.823005543979792\n",
      "Generated names\n",
      "----\n",
      " Velonah; Menlay; Rez; Kad; Ormie; Edye; Jenis; Liwge; Gelemadme; Men y \n",
      "----\n",
      "Epoch 52 average loss = 0.8255260716308235\n",
      "Generated names\n",
      "----\n",
      " Verrida; Agna; Agoro; Dorta; sorry; Fyansess; Marmeea; Sadie; Ferry; Anellle \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 average loss = 0.8211570768588556\n",
      "Generated names\n",
      "----\n",
      " Marcie; Cima; Bridlbe; Tyrme; Etole; Moiel; Egathan; Xollina; Nean; Labion \n",
      "----\n",
      "Epoch 54 average loss = 0.8225306480664223\n",
      "Generated names\n",
      "----\n",
      " Earre; Delina; Kontpurise; Naryr; Avanie; Groluboly; Clorel; Criggie; WaUli; ElWnie \n",
      "----\n",
      "Epoch 55 average loss = 0.8263127657205556\n",
      "Generated names\n",
      "----\n",
      " Kalene; Jenkyn; Aripsalot; Shlacon; Tarbeys; Tyllie; Earmile; Winno; Parma; Krisna \n",
      "----\n",
      "Epoch 56 average loss = 0.8187958617255415\n",
      "Generated names\n",
      "----\n",
      " Jethia; Egcgie; Meretl; Ferli; Erdie; Om; Tamoree; PriqMerlea; Resende; Jubbe \n",
      "----\n",
      "Epoch 57 average loss = 0.8172224882220839\n",
      "Generated names\n",
      "----\n",
      " Mardal; TohrOn; Anelo; Cee; CaIdlinga; Alaligym; Dolisa; Branne; Heafvoy; Algevet \n",
      "----\n",
      "Epoch 58 average loss = 0.8143497792197811\n",
      "Generated names\n",
      "----\n",
      " Tarthe; Sonelie; Leeb; EHerit; Yerfely; Waulye; Phricco; Shnyb; Jatos; Frusvia \n",
      "----\n",
      "Epoch 59 average loss = 0.8216676619202351\n",
      "Generated names\n",
      "----\n",
      " Dole; Egdano; Lorola; Steh; Mardeina; Win-falieur; Anadetta; Gelonis; Shy; Perria \n",
      "----\n",
      "Epoch 60 average loss = 0.8259132257971288\n",
      "Generated names\n",
      "----\n",
      " Eusandoav; Maibell; Metey; Jkondo; Choril; WernielS; Malminca; Tob; Frarelle; Darc \n",
      "----\n",
      "Epoch 61 average loss = 0.8180526951683099\n",
      "Generated names\n",
      "----\n",
      " Alyna; SNanne; Gasals; Sanco; Brearro; Lelita; Dele; Torrondia; Kata; Chelva \n",
      "----\n",
      "Epoch 62 average loss = 0.8157521895842059\n",
      "Generated names\n",
      "----\n",
      " lobin; Kainne; Jeesehton; Thonty; Kalce; Mareli; Jamine; Daner; Vallys; Jovah \n",
      "----\n",
      "Epoch 63 average loss = 0.8169613992102992\n",
      "Generated names\n",
      "----\n",
      " Vaveelle; Casid; Chree; Larna; Wiadoza; Cry; Choryde; Arrilena; Cran; Wisry \n",
      "----\n",
      "Epoch 64 average loss = 0.8185718983363203\n",
      "Generated names\n",
      "----\n",
      " Ders; Jiea; Jilpulena; Celanchra; Nareente; Moil; Samuna; Corild; Jughrine; Ath \n",
      "----\n",
      "Epoch 65 average loss = 0.8142847833395613\n",
      "Generated names\n",
      "----\n",
      " DaMeley; Orvea; Wei l; CraPe; Luspie; Garvelsa; Hekishe; Augyn; Verulland; Marmy \n",
      "----\n",
      "Epoch 66 average loss = 0.8229561887157429\n",
      "Generated names\n",
      "----\n",
      " KaCa; Romer; Morbel; Toahro; Lysgen; Mamylury; Ferregh; Ceelen; Farsurp; Colt \n",
      "----\n",
      "Epoch 67 average loss = 0.820339803522934\n",
      "Generated names\n",
      "----\n",
      " Miacha; Mixtellice; Kerr; Hean; Dannacte; Ledree; Katte; Avian; Austhus; Juliande \n",
      "----\n",
      "Epoch 68 average loss = 0.8142145839408396\n",
      "Generated names\n",
      "----\n",
      " Machiena; Lonneina; Rofothion; Tylle; Rasvee; Refee; Missil; Bertie; Tebalia; Austuk \n",
      "----\n",
      "Epoch 69 average loss = 0.8159083711950806\n",
      "Generated names\n",
      "----\n",
      " PaKre; Varnete; Denna; Flines; Jabucke; Etmelita; Lesila; Berh; Noraude; Nezerta \n",
      "----\n",
      "Epoch 70 average loss = 0.8181263087487638\n",
      "Generated names\n",
      "----\n",
      " Ordimef; Campy; Andciz; Parly; Karila; Gudan; Shannergigte; Lendinta; Rikeche; Kelyna \n",
      "----\n",
      "Epoch 71 average loss = 0.8125876441121336\n",
      "Generated names\n",
      "----\n",
      " Borsy; Cler; Mayallae; Retam; Gaxty; Tuj; Ura; Gillye; Andy; Maisa \n",
      "----\n",
      "Epoch 72 average loss = 0.8130936599999508\n",
      "Generated names\n",
      "----\n",
      " Marse; Sseyl; Dora; Kaban; Tsinshrone; elogestele; Amdias; Plamesta; Cericge; Agmeni \n",
      "----\n",
      "Epoch 73 average loss = 0.8147578057829962\n",
      "Generated names\n",
      "----\n",
      " Liom; Endays; Cynalyna; Ronelor; Enahanna; Fran; Clihmito; Flena; Rausett; Alahhga \n",
      "----\n",
      "Epoch 74 average loss = 0.8126140888945107\n",
      "Generated names\n",
      "----\n",
      " Gade; Neh; Chardelda; Dibba; Klerfrede; Dardencan; Milly; Jabana; Kathin; Phania \n",
      "----\n",
      "Epoch 75 average loss = 0.8064143093508048\n",
      "Generated names\n",
      "----\n",
      " Sifybada; Anasord; Juvy; Glibistte; Marilina; Diesten; Rotsharina; Palmean; Risebre; Kledisk \n",
      "----\n",
      "Epoch 76 average loss = 0.8118124015425023\n",
      "Generated names\n",
      "----\n",
      " Rungo; Gloan; Connetha; Labalyn; YHorifiaY; Hainne; Dekie; Ikoi; Goller; Alona \n",
      "----\n",
      "Epoch 77 average loss = 0.8086969006774042\n",
      "Generated names\n",
      "----\n",
      " Favine; Shica; Kydelr; Vatha; Kartha; Liny; Elystin; Karline; Taygee; Gytice \n",
      "----\n",
      "Epoch 78 average loss = 0.8064536469881604\n",
      "Generated names\n",
      "----\n",
      " Careicd; Syane; Kelinaline; Licka; Avanne; Cam; AeUl; Kaile; Almord; Cabadane \n",
      "----\n",
      "Epoch 79 average loss = 0.814769472174865\n",
      "Generated names\n",
      "----\n",
      " Joansere; Ollimona; Mugine; Conenotfey; Charabra; Teins; Jomia; Shatcha; Andua; Kan \n",
      "----\n",
      "Epoch 80 average loss = 0.8067067826578989\n",
      "Generated names\n",
      "----\n",
      " Ponnelf; Romard; Edusth; Etha; Corli; Con-ee; Wirnella; Berney; lifie; Hary \n",
      "----\n",
      "Epoch 81 average loss = 0.810682005631853\n",
      "Generated names\n",
      "----\n",
      " Mormatie; Arme; Bunnay; Tysabera; Farera; Purtson; Ulisi; Dunlon; Opzia; Narvonke \n",
      "----\n",
      "Epoch 82 average loss = 0.8119114511772507\n",
      "Generated names\n",
      "----\n",
      " Ruika; Jophana; Cinney; Therinta; Dandi; Meles; Crenia; Catyan; Ergfom; Kenncine \n",
      "----\n",
      "Epoch 83 average loss = 0.8090200227147423\n",
      "Generated names\n",
      "----\n",
      " CInna; Bettian; Belland; Assarie; Tafabolt; Arahbert; Bel; Oim; Cilfy; Torder \n",
      "----\n",
      "Epoch 84 average loss = 0.8079535372862244\n",
      "Generated names\n",
      "----\n",
      " almely; Maryeja; Entyatta; Cinshi; Shermin; Ednars; Derf-kanald; Sers; Bina; Quess \n",
      "----\n",
      "Epoch 85 average loss = 0.8099329380496528\n",
      "Generated names\n",
      "----\n",
      " Marris; Duvb; Shellirs; Sat nn; Jaiun; Phenros; Lidie; S Maalie; Glilnef-orv; Svelin \n",
      "----\n",
      "Epoch 86 average loss = 0.8070131186418338\n",
      "Generated names\n",
      "----\n",
      " Dorgaenn; Elenel; Ceanna; Shotina; Cottie; Nericie; Cootonce; Keline; Jusardi; LenEn \n",
      "----\n",
      "Epoch 87 average loss = 0.8087382993400472\n",
      "Generated names\n",
      "----\n",
      " Heista; La; Hencel; Elyna; Raurete; Starfina; Hetty; Clot; Madgy; Crissgon \n",
      "----\n",
      "Epoch 88 average loss = 0.8064744260223469\n",
      "Generated names\n",
      "----\n",
      " Hredia; Izey; Joaran; Simane; Sansela; Huddith; Yayabra; Shannet; Yonita; Janlen \n",
      "----\n",
      "Epoch 89 average loss = 0.8042427927445016\n",
      "Generated names\n",
      "----\n",
      " Jacahi; Eluell; Asida; Austa; Hestcen; Angonys; Ruscine; Gorillie; Gegren; Iosale \n",
      "----\n",
      "Epoch 90 average loss = 0.810597762361677\n",
      "Generated names\n",
      "----\n",
      " Wilris; Bandow; Ahritanne; Pybine; Lailla; Jethill; Sacilla; Woine; Sulika; Annyquestonus \n",
      "----\n",
      "Epoch 91 average loss = 0.8031373366703028\n",
      "Generated names\n",
      "----\n",
      " JoSter; Gelos; Rulpetha; Rily; Qui; Ja; Egina; Allita; Ansteth; Amry \n",
      "----\n",
      "Epoch 92 average loss = 0.805354818913408\n",
      "Generated names\n",
      "----\n",
      " Taorrect; Hehad; Barnon; Mileel; Dorberide; Giery; Grfina; Ninella; Phodganddor; Alishandie \n",
      "----\n",
      "Epoch 93 average loss = 0.8080732816103646\n",
      "Generated names\n",
      "----\n",
      " Lileleela; Feerladd; Guelina; Chor; eansele; Bethlem; Marnorel; Jisrye; Vigece; Shellyn \n",
      "----\n",
      "Epoch 94 average loss = 0.8121731407371588\n",
      "Generated names\n",
      "----\n",
      " Ennies; Nial; Nela; Byem; Sidgann; Marceie; Clissberinn; Hulasih; Rebiko; Emxy \n",
      "----\n",
      "Epoch 95 average loss = 0.799946312875296\n",
      "Generated names\n",
      "----\n",
      " Heanie; Finie; Kil; Jenethie; Judriange; Minna; Eghris; Supmer; Adie; Zovich \n",
      "----\n",
      "Epoch 96 average loss = 0.8045428316485859\n",
      "Generated names\n",
      "----\n",
      " Mavel; Carla; Bilbria; Arpie; Clistey; Vaselle; Demena; Caplon; Merryl; Kannen \n",
      "----\n",
      "Epoch 97 average loss = 0.8068469264043484\n",
      "Generated names\n",
      "----\n",
      " Bertha; Dypmie; Irdy; Helynna; Leied; LynU; Lorina; Wisnie; Elynta; Zaula \n",
      "----\n",
      "Epoch 98 average loss = 0.8037327750713628\n",
      "Generated names\n",
      "----\n",
      " Codee; ADon; Gily; Alisto; Doiee          t; Gelssia; Norv; Jensid; Jenolith; Doryane \n",
      "----\n",
      "Epoch 99 average loss = 0.8080862470385212\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=100\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print( \"Generated names\")\n",
    "    generate_sample(n_snippets=10)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_batch(names_ix,batch_size)\n",
    "        avg_cost += train(x, y)\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Charara; Fulivert; Chathalla; Rozie; Cath; Nighilo; MenAndre; Nannice; Ingey; Carlien; Katsie; Vichall; Jontina; Shelloy; Hender; Galiane; Clorolica; Berginke; Safo; Frow; Dolyandre; Morngia; Rorianne; Joitidro; Jopina; Hathly; Sham; Gerlee; Gaissse; Oifaina; Panny; Soe; Kevonda; Tenatna; urakia; Shabrom; wordye; Csoba; Chons; Lilylona; han; Charanis; Blical; Matbie; Bratca; Geromene; Caryandue; Caleeanna; Jussalie; Amna; AMiste; Nondian; Rsiante; Beinette; Kelm; Krann; Willa; Lol; Jeobe; Aurotha; Joffea; Malias; Rudmine; Mary; Mafi; cef; Herienn; Thype; Darny; Kant; Brty; Eleett; Nebore; Roauhta; Marbery; Nove; Thereste; Parm; Marecetlon; Wilban; Kase; Leanoth; Arthina; Silana; Sharmenn; Remann; Orine; Gellica; Karlie; Eloeth; Rvima; Nifgane; Harfera; Des-arack; FTapken; Amrie; Saabosene; Antansh; Omia; Cley \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(n_snippets=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Alliva; ArCoit; Ardia; Alicko; Aula; Amrrauh; Allimolpe; Arnnee; Anntha; Aletlorih \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(\" A\", n_snippets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Osnis; Ostaha; Osiar; Oslan; Ostey; Ospipre; Osba; Ostie; Osa; Osel \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(\" Os\", n_snippets=10, t=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework part 1 - generate questions (4 pts)\n",
    "\n",
    "* Apply recurrent neural networks to generate human-readable questions.\n",
    "* The dataset origins from https://www.kaggle.com/c/quora-question-pairs - a recent kaggle challenge.\n",
    "* The code below shows how to read the dataset\n",
    "* Please download the __train dataset__ from [here](https://www.kaggle.com/c/quora-question-pairs/data)\n",
    "* Avoid using test dataset as it contains artificially generated data.\n",
    "* Alternatively, pick any similar dataset you like.\n",
    "\n",
    "### [bonus] Word-level model (4+ points)\n",
    "\n",
    "Learn to generate questions on _word_ level, generating one word per RNN iteration.\n",
    "\n",
    "Kudos for \n",
    "* pre-training embedding layer with word2vec or similar\n",
    "* using more compute-efficient softmax functions (hierarchical or sampled softmax)\n",
    "* anything creative :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "df = pd.concat([df.question1,df.question2])\n",
    "\n",
    "questions = list(set(df))\n",
    "questions = filter(lambda x: type(x) is str, questions)\n",
    "start_token,end_token = \" \",\";\"\n",
    "questions = [start_token+name.replace(\";\",\",\")+end_token for name in questions]\n",
    "\n",
    "print('n samples = ',len(questions))\n",
    "for x in questions[::100000]:\n",
    "    print (x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
